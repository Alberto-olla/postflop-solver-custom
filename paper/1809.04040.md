Title: 1809.04040v3.pdf

URL Source: https://arxiv.org/pdf/1809.04040

Published Time: Sun, 22 Jan 2023 22:27:50 GMT

Number of Pages: 11

Markdown Content:
# Solving Imperfect-Information Games via Discounted Regret Minimization

# Noam Brown

Computer Science Department Carnegie Mellon University noamb@cs.cmu.edu

# Tuomas Sandholm

Computer Science Department Carnegie Mellon University sandholm@cs.cmu.edu

Abstract

> Counterfactual regret minimization (CFR) is a family of iter-ative algorithms that are the most popular and, in practice, fastest approach to approximately solving large imperfect-information games. In this paper we introduce novel CFR variants that 1) discount regrets from earlier iterations in var-ious ways (in some cases differently for positive and nega-tive regrets), 2) reweight iterations in various ways to ob-tain the output strategies, 3) use a non-standard regret min-imizer and/or 4) leverage â€œoptimistic regret matchingâ€. They lead to dramatically improved performance in many settings. For one, we introduce a variant that outperforms CFR+ , the prior state-of-the-art algorithm, in every game tested, includ-ing large-scale realistic settings. CFR+ is a formidable bench-mark: no other algorithm has been able to outperform it. Fi-nally, we show that, unlike CFR+, many of the important new variants are compatible with modern imperfect-information-game pruning techniques and one is also compatible with sampling in the game tree.

# Introduction

Imperfect-information games model strategic interactions between players that have hidden information, such as in negotiations, cybersecurity, and auctions. A common bench-mark for progress in this class of games is poker. The typi-cal goal is to find an (approximate) equilibrium in which no player can improve by deviating from the equilibrium. For extremely large imperfect-information games that cannot fit in a linear program of manageable size, typically iterative algorithms are used to approximate an equilibrium. A number of such iterative algorithms exist (Nesterov 2005; Hoda et al. 2010; Pays 2014; Kroer et al. 2015; Heinrich, Lanctot, and Silver 2015). The most popular ones are vari-ants of counterfactual regret minimization (CFR) (Zinke-vich et al. 2007; Lanctot et al. 2009; Gibson et al. 2012). In particular, the development of CFR+ was a key break-through that in many cases is at least an order of magni-tude faster than vanilla CFR (Tammelin 2014; Tammelin et al. 2015). CFR+ was used to essentially solve heads-up limit Texas holdâ€™em poker (Bowling et al. 2015) and was used to approximately solve heads-up no-limit Texas holdâ€™em (HUNL) endgames in Libratus , which defeated

> Copyright c Â©2019, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.

HUNL top professionals (Brown and Sandholm 2017c; Brown and Sandholm 2017b). A blend of CFR and CFR+ was used by DeepStack to defeat poker professionals in HUNL (MoravË‡ cÂ´ Ä±k et al. 2017). The best known theoretical bound on the number of it-erations needed for CFR and CFR+ to converge to an

-equilibrium (defined formally in the next section) is

O( 1

> 2

) (Zinkevich et al. 2007; Tammelin et al. 2015). This is asymptotically slower than first-order methods that converge at rate O( 1

> 

) (Hoda et al. 2010; Kroer et al. 2015). However, in practice CFR+ converges much faster than its theoretical bound, and even faster than O( 1

> 

) in many games. Nevertheless, we show in this paper that one can design new variants of CFR that significantly outperform CFR+. We show that CFR+ does relatively poorly in games where some actions are very costly mistakes (that is, they cause high regret in that iteration) and provide an intuitive example and explanation for this. To address this weakness, we intro-duce variants of CFR that do not assign uniform weight to each iteration. Instead, earlier iterations are discounted. As we show, this high-level idea can be instantiated in many dif-ferent ways. Furthermore, some combinations of our ideas perform significantly better than CFR+ while others perform worse than it. In particular, one variant outperforms CFR+ in every game tested.

# Notation and Background

We focus on sequential games as the most interesting and challenging application of this work, but our techniques also apply to non-sequential games. In an imperfect-information extensive-form (that is, tree-form) game there is a finite set of players, P. â€œNatureâ€ is also considered a player (rep-resenting chance) and chooses actions with a fixed known probability distribution. A state h is defined by all infor-mation of the current situation, including private knowledge known to only a subset of players. A(h) is the actions avail-able in a node and P (h) is the unique player who acts at that node. If action a âˆˆ A(h) leads from h to hâ€², then we write

h Â· a = hâ€². H is the set of all states in the game tree. Z âŠ† H

are terminal states for which no actions are available. For each player i âˆˆ P , there is a payoff function ui : Z â†’ R.We denote the range of payoffs in the game by âˆ†. Formally,

âˆ†i = max zâˆˆZ ui(z) âˆ’ min zâˆˆZ ui(z) and âˆ† = max iâˆˆP âˆ†i.Imperfect information is represented by information sets (infosets) for each player i âˆˆ P . For any infoset I belong-ing to player i, all states h, h â€² âˆˆ I are indistinguishable to player i. Every non-terminal state h âˆˆ H belongs to exactly one infoset for each player i. The set of actions that may be chosen in I is represented as A(I). We represent the set of all infosets belonging to player i where i acts by Ii.A strategy Ïƒi(I) is a probability vector over actions for player i in infoset I. The probability of a particular action a

is denoted by Ïƒi(I, a ). Since all states in an infoset belong-ing to player i are indistinguishable, the strategies in each of them are identical. Therefore, for any h âˆˆ I we define

Ïƒi(h, a ) = Ïƒi(I, a ) where i = P (h). We define Ïƒi to be a strategy for player i in every infoset in the game where player i acts. A strategy profile Ïƒ is a tuple of strategies, one per player. The strategy of every player other than i is repre-sented as Ïƒâˆ’i. ui(Ïƒi, Ïƒ âˆ’i) is the expected payoff for player

i if all players play according to strategy profile ã€ˆÏƒi, Ïƒ âˆ’iã€‰.

Ï€Ïƒ (h) = Î  hâ€²Â·avhÏƒP (hâ€²)(hâ€², a ) is the joint probability of reaching h if all players play according to Ïƒ. Ï€Ïƒi (h) is the contribution of player i to this probability (that is, the prob-ability of reaching h if all players other than i, and chance, always chose actions leading to h). Ï€Ïƒ

> âˆ’i

(h) is the contribu-tion of chance and all players other than i.A best response to Ïƒi is a strategy BR (Ïƒi) such that

ui

(Ïƒi, BR (Ïƒi)) = max Ïƒâ€²âˆ’i ui(Ïƒi, Ïƒ â€²âˆ’i). A Nash equilib-rium Ïƒâˆ— is a strategy profile where everyone plays a best re-sponse: âˆ€i, ui(Ïƒâˆ—

> i

, Ïƒ âˆ—âˆ’i) = max Ïƒâ€²

> i

ui(Ïƒâ€²

> i

, Ïƒ âˆ—âˆ’i) (Nash 1950). The exploitability e(Ïƒi) of a strategy Ïƒi in a two-player zero-sum game is how much worse it does versus a best re-sponse compared to a Nash equilibrium strategy. Formally,

e(Ïƒi) = ui

(Ïƒâˆ—

> i

, BR (Ïƒâˆ—

> i

)) âˆ’ ui

(Ïƒi, BR (Ïƒi)). In an -Nash equilibrium, no player has exploitability higher than .In CFR, the strategy vector for each infoset is determined according to a regret-minimization algorithm. Typically, re-gret matching (RM) is used as that algorithm within CFR due to RMâ€™s simplicity and lack of parameters. The expected value (or simply value ) to player i at state h given that all players play according to strategy profile Ïƒ from that point on is defined as vÏƒi (h). The value to i at infoset I where i acts is the weighted av-erage of the value of each state in the infoset, where the weight is proportional to iâ€™s belief that they are in that state conditional on knowing they are in I. For-mally, vÏƒ (I) = âˆ‘

> hâˆˆI

(Ï€Ïƒ

> âˆ’i

(h|I)vÏƒi (h)) and vÏƒ (I, a ) =

âˆ‘

> hâˆˆI

(Ï€Ïƒ

> âˆ’i

(h|I)vÏƒi (h Â· a)) where Ï€Ïƒ

> âˆ’i

(h|I) = Ï€Ïƒ

> âˆ’i(h)
> Ï€Ïƒ
> âˆ’i(I)

.Let Ïƒt be the strategy on iteration t. The instantaneous regret for action a in infoset I on iteration t is rt(I, a ) =

vÏƒt

(I, a ) âˆ’ vÏƒt

(I) and the regret on iteration T is

RT (I, a ) =

> T

âˆ‘

> t=1

rT (I, a ) (1) Additionally, RT

> +

(I, a ) = max {RT (I, a ), 0} and RT (I) = max a{RT

> +

(I, a )}. Regret for player i in the entire game is

RTi = max

> Ïƒâ€²
> i
> T

âˆ‘

> t=1

(ui(Ïƒâ€²

> i

, Ïƒ t

> âˆ’i

) âˆ’ ui(Ïƒti , Ïƒ t

> âˆ’i

)) (2) In RM, a player picks a distribution over actions in an infoset in proportion to the positive regret on those actions. Formally, on each iteration T + 1 , player i selects actions

a âˆˆ A(I) according to probabilities

ÏƒT +1 (I, a ) =

ï£±ï£²ï£³

> RT
> +(I,a )
> âˆ‘
> aâ€²âˆˆA(I)RT
> +(I,a â€²)

, if âˆ‘

> aâ€²

RT

> +

(I, a â€²) > 0

> 1
> |A(I)|

, otherwise (3) If a player plays according to regret matching in in-foset I on every iteration, then on iteration T , RT (I) â‰¤

âˆ†âˆš|A(I)|âˆšT (Cesa-Bianchi and Lugosi 2006). If a player plays according to CFR on every iteration, then

RTi â‰¤ âˆ‘

> IâˆˆI i

RT (I) (4) So, as T â†’ âˆž , RTi

> T

â†’ 0.The average strategy Â¯ÏƒTi (I) for an infoset I is

Â¯ÏƒTi (I) =

âˆ‘Tt=1

(Ï€Ïƒt

> i

(I)Ïƒti (I))âˆ‘Tt=1 Ï€Ïƒt

> i

(I) (5) CFR minimizes external regret (Zinkevich et al. 2007), so it converges to a coarse correlated equilibrium (Hart and Mas-Colell 2000). In two-player zero-sum games, this is also a Nash equilibrium. In two-player zero-sum games, if both playersâ€™ average regret satisfies RTi

> T

â‰¤ , then their av-erage strategies ã€ˆÂ¯ÏƒT

> 1

, Â¯ÏƒT

> 2

ã€‰ are a 2-Nash equilibrium (Waugh 2009). Thus, CFR is an anytime algorithm for finding an -Nash equilibrium in two-player zero-sum games. Although CFR theory calls for both players to simultane-ously update their regrets on each iteration, in practice far better performance is achieved by alternating which player updates their regrets on each iteration. However, this compli-cates the theory for convergence (Farina, Kroer, and Sand-holm ; Burch, Moravcik, and Schmid 2018). CFR+ is like CFR but with the following small changes. First, after each iteration any action with negative regret is set to zero regret. Formally, CFR+ chooses its strategy on iteration T + 1 according to Regret Matching+ (RM+) ,which is identical to Equation (3) but uses the regret-like value QT (I, a ) = max {0, Q T âˆ’1(I, a ) + rt(I, a )} rather than RT

> +

(I, a ). Second, CFR+ uses a weighted average strat-egy where iteration T is weighted by T rather than using a uniformly-weighted average strategy as in CFR. The best known convergence bound for CFR+ is higher (that is, worse in exploitability) than CFR by a constant factor of 2. Despite that, CFR+ typically converges much faster than CFR and usually even faster than O( 1

> 

).However, in some games CFR+ converges slower than

> 1
> T

. We now provide a two-player zero-sum game with this property. Consider the payoff matrix [ 1 0.9

> âˆ’0.71

] (where P1

chooses a row and P2 simultaneously chooses a column; the chosen entry in the matrix is the payoff for P1 while P2 re-ceives the opposite). We now proceed to introducing our im-provements to the CFR family. Weighted Averaging Schemes for CFR+

As described in the previous section, CFR+ traditionally uses â€œlinearâ€ averaging, in which iteration tâ€™s contribution to the average strategy is proportional to t. In this section we prove a bound for any sequence of non-decreasing weights when calculating the average strategy. However, the bound on convergence is never lower than that of vanilla CFR (that is, uniformly equal weight on the iterations).

Theorem 1. Suppose T iterations of RM+ are played in a two-player zero-sum game. Then the weighted aver-age strategy profile, where iteration t is weighed propor-tional to wt > 0 and wi â‰¤ wj for all i < j, is a

> wTâˆ‘Tt=1 wt

âˆ†|I| âˆš|A|âˆšT -Nash equilibrium.

The proof is in the appendix. It largely follows the proof for linear averaging in CFR+ (Tammelin et al. 2015). Empirically we observed that CFR+ converges faster when assigning iteration t a weight of t2 rather than a weight of t when calculating the average strategy. We therefore use this weight for CFR+ and its variants throughout this paper when calculating the average strategy.

# Regret Discounting for CFR and Its Variants

In all past variants of CFR, each iterationâ€™s contribution to the regrets is assigned equal weight. In this section we discuss discounting iterations in CFR when determining regretsâ€”in particular, assigning less weight to earlier iter-ations. This is very different from, and orthogonal to, the idea of discounting iterations when computing the average strategy, described in the previous section. To motivate discounting, consider the simple case of an agent deciding between three actions. The payoffs for the actions are 0, 1, and -1,000,000, respectively. From (3) we see that CFR and CFR+ assign equal probability to each ac-tion on the first iteration. This results in regrets of 333,333, 333,334, and 0, respectively. If we continue to run CFR or CFR+, the next iteration will choose the first and second ac-tion with roughly 50% probability each, and the regrets will be updated to be roughly 333,332.5 and 333,334.5, respec-tively. It will take 471,407 iterations for the agent to choose the second actionâ€”that is, the best actionâ€”with 100% prob-ability. Discounting the first iteration over time would dra-matically speed convergence in this case. While this might seem like a contrived example, many games include highly suboptimal actions. In this simple example the bad action was chosen on the first iteration, but in general bad actions may be chosen throughout a run, and discounting may be useful far beyond the first few iterations. Discounting prior iterations has received relatively lit-tle attention in the equilibrium-finding community. â€œOpti-misticâ€ regret minimizing variants exist that assign a higher weight to recent iterations, but this extra weight is tempo-rary and typically only applies to a short window of re-cent iterations; for example, counting the most recent it-erate twice (Syrgkanis et al. 2015). We investigate opti-mistic regret minimizers as part of CFR later in this paper. CFR+ discounts prior iterationsâ€™ contribution to the aver-age strategy , but not the regrets . Discounting prior iterations has also been used in CFR for situations where the game structure changes, for example due to interleaved abstrac-tion and equilibrium finding (Brown and Sandholm 2014; Brown and Sandholm 2015b). There has also been some work on applying discounting to perfect-information game solving in Monte Carlo Tree Search (Hashimoto et al. 2011). Outside of equilibrium finding, prior research has ana-lyzed the theory for discounted regret minimization (Cesa-Bianchi and Lugosi 2006). That work investigates applying RM (and other regret minimizers) to a sequence of itera-tions in which iteration t has weight wt (assuming wt â‰¤ 1

and the final iteration has weight 1). For RM, it proves that if âˆ‘âˆž

> t=1

wt = âˆž then weighted average regret, defined as

Rw,T i = max aâˆˆA

> âˆ‘Tt=1 (wtrt(a))
> âˆ‘Tt=1 wt

is bounded by

Rw,T i â‰¤ âˆ†âˆš|A|

âˆšâˆ‘Tt=1 w2

> t

âˆ‘Tt=1 wt

(6) Prior work has shown that, in two-player zero-sum games, if weighted average regret is , then the weighted average strat-egy, defined as Ïƒw,T i (I) =

> âˆ‘
> tâˆˆT

(wtÏ€Ïƒti (I)Ïƒti (I)

)

> âˆ‘
> tâˆˆT(wtÏ€Ïƒti(I))

for infoset

I, is a 2-Nash equilibrium (Brown and Sandholm 2014). While there are a limitless number of discounting schemes that converge in theory, not all of them perform well in practice. This paper introduces a number of variants that perform particularly well also in practice. The first algo-rithm, which we refer to as linear CFR (LCFR) , is identical to CFR, except on iteration t the updates to the regrets and average strategies are given weight t. That is, the iterates are weighed linearly. (Equivalently, one could multiply the accumulated regret by tt+1 on each iteration. We do this in our experiments to reduce the risk of numerical instability.) This means that after T iterations of LCFR, the first itera-tion only has a weight of 2

> T2+T

on the regrets rather than a weight of 1

> T

, which would be the case in CFR and CFR+. In the motivating example introduced at the beginning of this section, LCFR chooses the second action with 100% proba-bility after only 970 iterations while CFR+ requires 471,407 iterations. Furthermore, from (6), the theoretical bound on the convergence of regret is only greater than vanilla CFR by a factor of 2âˆš3 . One could more generally use any poly-nomial weighting of t.Since the changes from CFR that lead to LCFR and CFR+ do not conflict, it is natural to attempt to combine them into a single algorithm that weighs each iteration t proportional to

t and also has a floor on regret at zero like CFR+. However, we empirically observe that this algorithm, which we refer to as LCFR+ , actually leads to performance that is worse than LCFR and CFR+ in the games we tested, even though its theoretical bound on convergence is the same as for LCFR. Nevertheless, we find that using a less-aggressive dis-counting scheme leads to consistently strong performance. We can consider a family of algorithms called Discounted CFR with parameters Î± Î² , and Î³ (DCFR Î±,Î²,Î³ ), defined by multiplying accumulated positive regrets by tÎ±

> tÎ±+1

, negative regrets by tÎ²

> tÎ²+1

, and contributions to the average strategy by

( tt+1 )Î³ on each iteration t. In this case, LCFR is equivalent to DCFR 1,1,1, because multiplying iteration tâ€™s regret and contribution to the average strategy by tâ€²

> tâ€²+1

on every itera-tion t â‰¤ tâ€² < T is equivalent to weighing iteration t by tT .CFR+ (where iteration tâ€™s contribution to the average strat-egy is proportional to t2) is equivalent to DCFR âˆž,âˆ’âˆž ,2.In preliminary experiments we found the optimal choice of Î±, Î², and Î³ varied depending on the specific game. How-ever, we found that setting Î± = 3 /2, Î² = 0 , and Î³ = 2 led to performance that was consistently stronger than CFR+. Thus, when we refer to DCFR with no parameters listed, we assume this set of parameters are used. Theorem 2 shows that DCFR has a convergence bound that differs from CFR only by a constant factor.

Theorem 2. Assume that T iterations of DCFR are con-ducted in a two-player zero-sum game. Then the weighted average strategy profile is a 6âˆ† |I| (|âˆš|A|+ 1âˆšT )/âˆšT -Nash equilibrium.

We provide the proof in the appendix. It combines el-ements of the proof for CFR+ (Tammelin et al. 2015) and the proof that discounting in regret minimization is sound (Cesa-Bianchi and Lugosi 2006). One of the drawbacks of setting Î² â‰¤ 0 is that suboptimal actions (that is, actions that have an expected value lower than some other action in every equilibrium) no longer have regrets that approach âˆ’âˆž over time. Instead, for Î² = 0 they will approach some constant value and for Î² < 0 they will approach 0. This makes the algorithm less compatible with improvements that prune negative-regret actions (Brown and Sandholm 2015a; Brown and Sandholm 2017a). Such prun-ing algorithms can lead to more than an order of magnitude reduction in computational and space requirements for some games. Setting Î² > 0 better facilitates this pruning. For this reason in our experiments we also show results for Î² = 0 .5.

# Experimental setup

We now introduce the games used in our experiments.

Description of heads-up no-limit Texas holdâ€™em

We conduct experiments on subgames of HUNL poker, a primary benchmark for imperfect-information game solv-ing. In the version of HUNL we use, and which is standard in the Annual Computer Poker Competition, the two players (P1 and P2) start each hand with $20,000. The players alter-nate positions after each hand. On each of the four rounds of betting, each player can choose to either fold, call, or raise. Folding results in the player losing and the money in the pot being awarded to the other player. Calling means the player places a number of chips in the pot equal to the opponentâ€™s share. Raising means the player adds more chips to the pot than the opponentâ€™s share. A round ends when a player calls (if both players have acted). Players cannot raise beyond the $20,000 they start with. All raises must be at least $100 and at least as larger as any previous raise on that round. At the start of each hand of HUNL, both players are dealt two private cards from a standard 52-card deck. P1 places $100 in the pot and P2 places $50 in the pot. A round of betting then occurs. Next, three community cards are dealt face up. Another round of betting occurs, starting with P1.After the round is over, another community card is dealt face up, and another round of betting starts with P1 acting first. Finally, one more community card is revealed and a final betting round occurs starting with P1. Unless a player has folded, the player with the best five-card poker hand, con-structed from their two private cards and the five community cards, wins the pot. In the case of a tie, the pot is split evenly. Although the HUNL game tree is too large to traverse completely without sampling, state-of-the-art agents for HUNL solve subgames of the full game in real time during play (Brown and Sandholm 2017b; MoravË‡ cÂ´ Ä±k et al. 2017; Brown and Sandholm 2017c; Brown, Sandholm, and Amos 2018) using a small number of the available bet sizes. For ex-ample, Libratus solved in real time the remainder of HUNL starting on the third betting round. We conduct our HUNL experiments on four subgames generated by Libratus 1. The subgames were selected prior to testing. Although the inputs to the subgame are publicly available (the beliefs of both players at the start of the subgame about what state they are in, the number of chips in the pot, and the revealed cards), the exact bet sizes that Libratus considered have not been publicly revealed. We therefore use the bet sizes of 0.5x and 1x the size of the pot, as well as an all-in bet (betting all re-maining chips) for the first bet of each round. For subsequent bets in a round, we consider 1x the pot and all-in. Subgame 1 begins at the start of the third betting round and continues to the end of the game. There are $500 in the pot at the start of the round. This is the most common situ-ation to be in upon reaching the third betting round, and is also the hardest for AIs to solve because the remaining game tree is the largest. Since there is only $500 in the pot but up to $20,000 could be lost, this subgames contains a number of high-penalty mistake actions. Subgame 2 begins at the start of the third betting round and has $4,780 in the pot at the start of the round. Subgame 3 begins at the start of the fourth (and final) betting round with $500 in the pot, which is a common situation. Subgame 4 begins at the start of the fourth betting round with $3,750 in the pot. Exploitability is measured in terms of milli big blinds per game (mbb/g), a standard measurement in the field, which represents the number of big blinds ( P1â€™s original contribution to the pot) lost per hand of poker multiplied by 1,000.

Description of Goofspiel

In addition to HUNL subgames, we also consider a ver-sion of the game of Goofspiel (limited to just five cards per player). In this version of Goofspiel, each player has five hidden cards in their hand (A, 2, 3, 4, and 5), with A being valued as 1. A deck of five cards (also of rank A, 2, 3, 4, and 5), is placed between the two players. In the variant we con-sider, both players know the order of revealed cards in the center will be A, 2, 3, 4, 5. On each round, the top card of the deck is flipped and is considered the prize card. Each player then simultaneously plays a card from their hand. The player who played the higher-ranked card wins the prize card. If the players played the same rank, then they split the prizeâ€™s value. The cards that were bid are discarded. At the end of

> 1https://github.com/CMU-EM/LibratusEndgames

the game, players add up the ranks of their prize cards. A playerâ€™s payoff is the difference between his total value and the total value of his opponent.

# Experiments on Regret Discounting and Weighted Averaging

Our experiments are run for 32,768 iterations for HUNL subgames and 8,192 iterations for Goofspiel. Since all the algorithms tested only converge to an -equilibrium rather than calculating an exact equilibrium, it is up to the user to decide when a solution is sufficiently converged to termi-nate a run. In practice, this is usually after 100 - 1,000 iter-ations (Brown and Sandholm 2017c; MoravË‡ cÂ´ Ä±k et al. 2017). For example, an exploitability of 1 mbb/g is considered suf-ficiently converged so as to be essentially solved (Bowling et al. 2015). Thus, the performance of the presented algo-rithms between 100 and 1,000 iterations is arguably more important than the performance beyond 10,000 iterations. Nevertheless, we show performance over a long time hori-zon to display the long-term behavior of the algorithms. All our experiments use the alternating-updates form of CFR. We measure the average exploitability of the two players. Our experiments show that LCFR can dramatically im-prove performance over CFR+ over reasonable time hori-zons in certain games. However, asymptotically, LCFR ap-pears to do worse in practice than CFR+. LCFR does par-ticularly well in subgame 1 and 3, which (due to the small size of the pot relative to the amount of money each player can bet) have more severe mistake actions compared to sub-games 2 and 4. It also does poorly in Goofspiel, which also likely does not have severely suboptimal actions. This sug-gests that LCFR is particularly well suited for games with the potential for large mistakes. Our experiments also show that DCFR 32 ,0,2 matches or outperforms CFR+ across the board. The improvement is usually a factor of 2 or 3. In Goofspiel, DCFR 32 ,0,2 results in essentially identical performance as CFR+. DCFR 32 ,âˆ’âˆž ,2, which sets negative regrets to zero rather than multiplying them by 12 each iteration, generally also leads to equally strong performance, but in rare cases (such as in Figure 2), can produce a spike in exploitability that takes many iterations to recover from. Thus, we generally recommend using DCFR 32 ,0,2 over DCFR 32 ,âˆ’âˆž ,2.DCFR 32 , 12 ,2 multiplies negative regrets by âˆštâˆšt+1 on itera-tion t, which allows suboptimal actions to decrease in regret to âˆ’âˆž and thereby facilitates algorithms that temporarily prune negative-regret sequences. In the HUNL subgames, DCFR 32 , 12 ,2 performed very similarly to DCFR 32 ,0,2. How-ever, in Goofspiel it does noticeably worse. This suggests that DCFR 32 , 12 ,2 may be preferable to DCFR 32 ,0,2 in games with large mistakes when a pruning algorithm may be used, but that DCFR 32 ,0,2 should be used otherwise.

# NormalHedge for CFR Variants

CFR is a framework for applying regret minimization in-dependently at each infoset in the game. Typically RM is

Figure 1: Convergence in HUNL Subgame1.

Figure 2: Convergence in HUNL Subgame2.

Figure 3: Convergence in HUNL Subgame 3. used as the regret minimizer primarily due to its lack of pa-rameters and its simple implementation. However, any re-gret minimizer can be applied. Previous research investi-gated using Hedge (Littlestone and Warmuth 1994; Freund and Schapire 1997) in CFR rather than RM (Brown, Kroer, and Sandholm 2017). This led to better performance in small games, but worse performance in large games. In this section we investigate instead using NormalHedge (NH) (Chaud-huri, Freund, and Hsu 2009) as the regret minimizer in CFR. In NH, on each iteration T + 1 a player i selects actions Figure 4: Convergence in HUNL Subgame 4.

Figure 5: Convergence in 5-card Goofspiel variant.

a âˆˆ A(I) proportional to RT

> +(I,a )
> ct

exp ( (RT

> +(I,a )) 2
> 2ct

) where

ct > 0 satisfies 1

> N

âˆ‘Ni=1 exp ( (RT

> +(I,a )) 2
> 2ct

) = e. If a player plays according to NH in infoset I, then cumulative regret for that infoset is at most O(âˆ† âˆšT ln( |A|) + âˆ† ln 2(|A|)) .NH shares two desirable properties with RM: it does not have any parameters and it assigns zero probability to ac-tions with negative regret (which means it can be easily used in CFR+ with a floor on regret at zero). However, the NH op-eration is more computationally expensive than RM because it involves exponentiation and a line search for ct.In our experiments we investigate using NH in place of RM for DCFR 32 ,0,2 and in place of RM for LCFR. We found that NH did worse in all HUNL subgames compared to RM in LCFR, so we omit those results. Figure 6 and Figure 8 shows that NH outperforms RM in HUNL subgames when combined with DCFR 32 ,0,2. However, it does worse than RM in Figure 7 and Figure 9. The two subgames it does better in have the largest â€œmistakeâ€ actions, which suggest NH may do better in games that have large mistake actions. In these experiments the performance of NH is measured in terms of exploitability as a function of number of itera-tions. However, in our implementation, each iteration takes five times longer due to the exponentiation and line search operations involved in NH. Thus, using NH actually slows convergence in practice. Nevertheless, NH may be prefer-able in certain situations where the cost of the exponenti-ation and line search operations are insignificant, such as when an algorithm is bottlenecked by memory access rather than computational speed.

# Optimistic CFR Variants

Optimistic Hedge (Syrgkanis et al. 2015) is a regret min-imization algorithm similar to Hedge in which the last it-eration is counted twice when determining the strategy for the next iteration. This can lead to substantially faster con-vergence, including in some cases an improvement over the

O( 1

> 2

) bound on regret of typical regret minimizers. We investigate counting the last iteration twice when cal-culating the strategy for the next iteration (Burch 2017). Formally, when applying Equation (3) to determine the strategy for the next iteration, we use a modified regret

RT

> mod

(I, a ) = âˆ‘T âˆ’1

> t=1

rt(I, a ) + 2 rT (I, a ) in the equation in place of RT (I, a ). We refer to this as Optimistic RM, and any CFR variant that uses it as Optimistic. We found that Optimistic DCFR 32 ,0,2 did worse than DCFR 32 ,0,2 in all HUNL subgames, so we omit those results. Figure 6 and Figure 8 shows that Optimistic LCFR outperforms LCFR in two HUNL subgames. However, it does worse than LCFR in Figure 7 and Figure 9. Just as in the case of NH, the two subgames that Optimistic LCFR does better in have the largest â€œmistakeâ€ actions, which suggests that Optimistic LCFR may do better than LCFR in games that have large mistake actions. These are the same situations that LCFR normally excels in, so this suggests that in a situation where LCFR is preferable, one may wish to use Optimistic LCFR.

Figure 6: Convergence in HUNL Subgame 1.

# Discounted Monte Carlo CFR

Monte Carlo CFR (MCCFR) is a variant of CFR in which certain player actions or chance outcomes are sam-pled (Lanctot et al. 2009; Gibson et al. 2012). MCCFR com-bined with abstraction has produced state-of-the-art HUNL poker AIs (Brown and Sandholm 2017c). It is also partic-ularly useful in games that do not have a special structure that can be exploited to implement a fast vector-based im-plementation of CFR (Lanctot et al. 2009; Johanson et al. Figure 7: Convergence in HUNL Subgame 2.

Figure 8: Convergence in HUNL Subgame 3.

Figure 9: Convergence in HUNL Subgame 4. 2011). There are many forms of MCCFR with different sam-pling schemes. The most popular is external-sampling MC-CFR, in which opponent and chance actions are sampled ac-cording to their probabilities, but all actions belonging to the player updating his regret are traversed. Other MCCFR vari-ants exist that achieve superior performance (Jackson 2017), but external-sampling MCCFR is simple and widely used, which makes it useful as a benchmark for our experiments.

Figure 10: Convergence of MCCFR in HUNL Subgame 3.

Figure 11: Convergence of MCCFR in HUNL Subgame 4. Although CFR+ provides a massive improvement over CFR in the unsampled case, the changes present in CFR+ (a floor on regret at zero and linear averaging), do not lead to superior performance when applied to MCCFR (Burch 2017). In contrast, in this section we show that the changes present in LCFR do lead to superior performance when applied to MCCFR. Specifically, we divide the MCCFR run into periods of 10 7 nodes touched. Nodes touched is an implementation-independent and hardware-independent proxy for time that counts the number of nodes traversed (in-cluding terminal nodes). After each period n ends, we multi-ply all accumulated regrets and contributions to the average strategies by nn+1 . Figure 10 and Figure 11 demonstrate that this leads to superior performance in HUNL compared to vanilla MCCFR. The improvement is particularly noticeable in subgame 3, which features the largest mistake actions. We also show performance if one simply multiplies the accumu-lated regrets and contributions to the average strategy by 110

after the first period ends, and thereafter runs vanilla MC-CFR (the â€œInitial Discount MCCFRâ€ variant). The displayed results are the average of 100 different runs.

# Conclusions

We introduced variants of CFR that discount prior itera-tions, leading to stronger performance than the prior state-of-the-art CFR+, particularly in settings that involve large mistakes. In particular, the DCFR 32 ,0,2 variant matched or outperformed CFR+ in all settings. Acknowledgments

This material is based on work supported by the National Science Foundation under grants IIS-1718457, IIS-1617590, and CCF-1733556, and the ARO under award W911NF-17-1-0082. Noam is also sponsored by an Open Philanthropy Project AI Fellowship and a Tencent AI Lab Fellowship.

# References

[Bowling et al. 2015] Bowling, M.; Burch, N.; Johanson, M.; and Tammelin, O. 2015. Heads-up limit holdâ€™em poker is solved. Science 347(6218):145â€“149. [Brown and Sandholm 2014] Brown, N., and Sandholm, T. 2014. Regret transfer and parameter optimization. In AAAI ,594â€“601. [Brown and Sandholm 2015a] Brown, N., and Sandholm, T. 2015a. Regret-based pruning in extensive-form games. In

NIPS , 1972â€“1980. [Brown and Sandholm 2015b] Brown, N., and Sandholm, T. 2015b. Simultaneous abstraction and equilibrium finding in games. In International Joint Conference on Artificial Intel-ligence (IJCAI) .[Brown and Sandholm 2017a] Brown, N., and Sandholm, T. 2017a. Reduced space and faster convergence in imperfect-information games via pruning. In International Conference on Machine Learning .[Brown and Sandholm 2017b] Brown, N., and Sandholm, T. 2017b. Safe and nested subgame solving for imperfect-information games. In Advances in Neural Information Pro-cessing Systems , 689â€“699. [Brown and Sandholm 2017c] Brown, N., and Sandholm, T. 2017c. Superhuman AI for heads-up no-limit poker: Libra-tus beats top professionals. Science eaao1733. [Brown, Kroer, and Sandholm 2017] Brown, N.; Kroer, C.; and Sandholm, T. 2017. Dynamic thresholding and pruning for regret minimization. In AAAI Conference on Artificial Intelligence (AAAI) , 421â€“429. [Brown, Sandholm, and Amos 2018] Brown, N.; Sandholm, T.; and Amos, B. 2018. Depth-limited solving for imperfect-information games. In Advances in Neural Information Pro-cessing Systems .[Burch, Moravcik, and Schmid 2018] Burch, N.; Moravcik, M.; and Schmid, M. 2018. Revisiting cfr+ and alternating updates. arXiv preprint arXiv:1810.11542 .[Burch 2017] Burch, N. 2017. Time and Space: Why Im-perfect Information Games are Hard . Ph.D. Dissertation, University of Alberta. [Cesa-Bianchi and Lugosi 2006] Cesa-Bianchi, N., and Lu-gosi, G. 2006. Prediction, learning, and games . Cambridge University Press. [Chaudhuri, Freund, and Hsu 2009] Chaudhuri, K.; Freund, Y.; and Hsu, D. J. 2009. A parameter-free hedging algo-rithm. In Advances in neural information processing sys-tems , 297â€“305. [Farina, Kroer, and Sandholm ] Farina, G.; Kroer, C.; and Sandholm, T. Online convex optimization for sequential de-cision processes and extensive-form games. In AAAI Con-ference on Artificial Intelligence (AAAI) .[Freund and Schapire 1997] Freund, Y., and Schapire, R. 1997. A decision-theoretic generalization of on-line learn-ing and an application to boosting. Journal of Computer and System Sciences .[Gibson et al. 2012] Gibson, R.; Lanctot, M.; Burch, N.; Szafron, D.; and Bowling, M. 2012. Generalized sampling and variance in counterfactual regret minimization. In AAAI Conference on Artificial Intelligence , 1355â€“1361. [Hart and Mas-Colell 2000] Hart, S., and Mas-Colell, A. 2000. A simple adaptive procedure leading to correlated equilibrium. Econometrica 68:1127â€“1150. [Hashimoto et al. 2011] Hashimoto, J.; Kishimoto, A.; Yoshizoe, K.; and Ikeda, K. 2011. Accelerated UCT and its application to two-player games. In Advances in Computer Games , 1â€“12. Springer. [Heinrich, Lanctot, and Silver 2015] Heinrich, J.; Lanctot, M.; and Silver, D. 2015. Fictitious self-play in extensive-form games. In ICML , 805â€“813. [Hoda et al. 2010] Hoda, S.; Gilpin, A.; PeËœ na, J.; and Sand-holm, T. 2010. Smoothing techniques for computing Nash equilibria of sequential games. Mathematics of Operations Research 35(2):494â€“512. Conference version appeared in WINE-07. [Jackson 2017] Jackson, E. 2017. Targeted CFR. In AAAI Workshop on Computer Poker and Imperfect Information .[Johanson et al. 2011] Johanson, M.; Waugh, K.; Bowling, M.; and Zinkevich, M. 2011. Accelerating best response calculation in large extensive games. In Proceedings of the International Joint Conference on Artificial Intelligence (IJ-CAI) , 258â€“265. [Kroer et al. 2015] Kroer, C.; Waugh, K.; KÄ±lÄ±nc Â¸-Karzan, F.; and Sandholm, T. 2015. Faster first-order methods for extensive-form game solving. In Proceedings of the ACM Conference on Economics and Computation (EC) , 817â€“834. ACM. [Lanctot et al. 2009] Lanctot, M.; Waugh, K.; Zinkevich, M.; and Bowling, M. 2009. Monte Carlo sampling for regret minimization in extensive games. In Proceedings of the An-nual Conference on Neural Information Processing Systems (NIPS) , 1078â€“1086. [Littlestone and Warmuth 1994] Littlestone, N., and War-muth, M. K. 1994. The weighted majority algorithm. Infor-mation and Computation 108(2):212â€“261. [MoravË‡ cÂ´ Ä±k et al. 2017] MoravË‡ cÂ´ Ä±k, M.; Schmid, M.; Burch, N.; LisÂ´ y, V.; Morrill, D.; Bard, N.; Davis, T.; Waugh, K.; Jo-hanson, M.; and Bowling, M. 2017. Deepstack: Expert-level artificial intelligence in heads-up no-limit poker. Science .[Nash 1950] Nash, J. 1950. Equilibrium points in n-person games. Proceedings of the National Academy of Sciences

36:48â€“49. [Nesterov 2005] Nesterov, Y. 2005. Excessive gap technique in nonsmooth convex minimization. SIAM Journal of Opti-mization 16(1):235â€“249. [Pays 2014] Pays, F. 2014. An interior point approach to large games of incomplete information. In AAAI Computer Poker Workshop .[Syrgkanis et al. 2015] Syrgkanis, V.; Agarwal, A.; Luo, H.; and Schapire, R. E. 2015. Fast convergence of regularized learning in games. In Neural Information Processing Sys-tems , 2989â€“2997. [Tammelin et al. 2015] Tammelin, O.; Burch, N.; Johanson, M.; and Bowling, M. 2015. Solving heads-up limit texas holdâ€™em. In IJCAI .[Tammelin 2014] Tammelin, O. 2014. Solving large im-perfect information games using cfr+. arXiv preprint arXiv:1407.5042 .[Waugh 2009] Waugh, K. 2009. Abstraction in large exten-sive games. Masterâ€™s thesis, University of Alberta. [Zinkevich et al. 2007] Zinkevich, M.; Johanson, M.; Bowl-ing, M. H.; and Piccione, C. 2007. Regret minimization in games with incomplete information. In Neural Information Processing Systems (NIPS) , 1729â€“1736. Appendix

# Proof of Theorem 1

Consider the weighted sequence of iterates Ïƒâ€²1, ..., Ïƒ â€²T in which Ïƒâ€²t is identical to Ïƒt, but weighed by wt. The regret of action a in infoset I on iteration t of this new sequence is

Râ€²t(I, a ).From Lemma 2 we know that Rt(I, a ) â‰¤ âˆ†âˆš|A|âˆšT

for player i for action a in infoset I. Since wa,t is a non-decreasing sequence, so we can apply Lemma 1 us-ing weight wt for iteration t with B = âˆ† âˆš|A|âˆšT and

C = 0. From Lemma 1, this means that Râ€²t(I, a ) â‰¤

wT âˆ†âˆš|A|âˆšT . Applying (4), we get weighted regret is at most wT âˆ†|I i|âˆš|A|âˆšT for player i. Thus, weighted average regret is at most wT âˆ†|I i|âˆš|A|âˆšT

> âˆ‘Tt=1 wt

. Since |I 1| +

|I 2| = |I| , so the weighted average strategies form a

> wTâˆ†|I|

âˆš|A|âˆšT

> âˆ‘Tt=1 wt

-Nash equilibrium.

# Proof Theorem 2

Proof. Since the lowest amount of instantaneous regret on any iteration is âˆ’âˆ† and DCFR multiplies negative regrets by 12 each iteration, so regret for any action at any point is greater than âˆ’2âˆ† .Consider the weighted sequence of iterates Ïƒâ€²1, ..., Ïƒ â€²T

in which Ïƒâ€²t is identical to Ïƒt, but weighed by wa,t =Î T âˆ’1

> i=ti2
> (i+1) 2

= 6t2

> T(T+1)(2 T+1)

rather than wt =Î T âˆ’1

> i=ti3/2
> i3/2+1

. The regret of action a in infoset I on iteration

t of this new sequence is Râ€²t(I, a ).From Lemma 4 we know that Rt(I, a ) â‰¤ 2âˆ† âˆš|A|âˆšT

for player i for action a in infoset I. Since wa,t is an increasing sequence, so we can apply Lemma 1 using weight wa,t for iteration t with B = 2âˆ† âˆš|A|âˆšT and

C = âˆ’2âˆ† . From Lemma 1, this means that Râ€²t(I, a ) â‰¤

> 6T2(2âˆ†

âˆš|A|âˆšT +2âˆ†)

(T (T +1)(2 T +1)

) â‰¤ 6âˆ†( |âˆš|A| + 1âˆšT )/âˆšT . Applying (4), we get weighted regret is at most 6âˆ† |I i|(|âˆš|A| +

> 1âˆšT

)/âˆšT . Since the weights sum to one, this is also weighted average regret. Since |I 1| + |I 2| = |I| , so the weighted average strategies form a 6âˆ†( |I| (âˆš|A| +

> 1âˆšT

)/âˆšT -Nash equilibrium.

Lemma 1. Call a sequence x1, ..., x T of bounded real val-ues BC -plausible if B > 0, C â‰¤ 0, âˆ‘it=1 xt â‰¥ C for all i, and âˆ‘Tt=1 xt â‰¤ B. For any BC -plausible sequence and any sequence of non-decreasing weights wt â‰¥ 0,

âˆ‘Tt=1 (wtxt) â‰¤ wT (B âˆ’ C).Proof. The lemma closely resembles Lemma 3 from (Tam-melin et al. 2015) and the proof shares some elements. We construct a BC -plausible sequence xâˆ—

> 1

, ..., x âˆ—

> T

that maximizes the weighted sum. That is, âˆ‘Tt=1 wtxâ€²

> t

=max xâ€²

> 1,...,x â€²
> T

âˆ‘Tt=1 wtxâ€²

> t

. We show that xâˆ—

> 1

= C, xâˆ—

> t

= 0

for 1 < t < T , and xâˆ—

> T

= ( B âˆ’ C).Consider xâˆ—

> T

. Clearly in order to maximize the weighted sum, xâˆ—

> T

= B âˆ’ âˆ‘T âˆ’1

> t=1

(wtxâˆ—

> t

). Next, consider xâˆ—

> t

for t < T

and assume xâˆ—

> tâ€²

= C âˆ’ âˆ‘tâ€²

> t=1

(wtxâˆ—

> t

) for t < tâ€² < T

and assume xâˆ—

> T

= B âˆ’ âˆ‘T âˆ’1

> t=1

(wtxâˆ—

> t

). Since wt â‰¤ wT

and wt â‰¤ wtâ€² , so âˆ‘Ti=t(wixâˆ—

> i

) would be maximized if

xâˆ—

> t

= C âˆ’ âˆ‘tâˆ’1

> i=1

(wixâˆ—

> i

). By induction, this means xâˆ—

> 1

= C,

xâˆ—

> t

= 0 for 1 < t < T , and xâˆ—

> T

= B âˆ’ C. In this case

âˆ‘Tt=1 (wtxâˆ—

> t

) â‰¤ wT (B âˆ’ C) + w1C â‰¤ wT (B âˆ’ C). Since

xâˆ— is a maximizing sequence, so for any sequence x we have that âˆ‘Tt=1 (wtxt) â‰¤ wT (B âˆ’ C).

Lemma 2. Given a sequence of strategies Ïƒ1, ..., Ïƒ T , each defining a probability distribution over a set of actions A,consider any definition for Qt(a) satisfying the following conditions: 1. Q0(a) = 0

2. Qt(a) = Qtâˆ’1(a) + rt(a) if Qtâˆ’1(a) + rt(a) > 0

3. 0 â‰¥ Qt(a) â‰¥ Qtâˆ’1(a) + rt(a) if Qtâˆ’1(a) + rt(a) â‰¤ 0

The regret-like value Qt(a) is then an upper bound on the regret Rt(a) and Qt(a) âˆ’ Qtâˆ’1(a) â‰¥ rt(a) = Rt(a) âˆ’

Rtâˆ’1(a).Proof. The lemma and proof closely resemble Lemma 1 in (Tammelin et al. 2015). For any t â‰¥ 1 we have Qt+1 (a) âˆ’

Qt(a) â‰¥ Qt(a) + rt+1 (a) âˆ’ Qt(a) = Rt+1 (a) âˆ’ Rt(a).Since Q0(a) = 0 and R0(a) = 0 , so Qt(a) â‰¥ Rt(a).

Lemma 3. Given a set of actions A and any sequence of rewards vt such that |vt(a) âˆ’ vt(b)| â‰¤ âˆ† for all t and all

a, b âˆˆ A, after playing a sequence of strategies determined by regret matching but using the regret-like value Qt(a) in place of Rt(a), QT (a) â‰¤ âˆ†âˆš|A|T for all a âˆˆ A.Proof. The proof is identical to that of Lemma 2 in (Tam-melin et al. 2015).

Lemma 4. Assume that player i conducts T iterations of DCFR. Then weighted regret for the player is at most

âˆ†|I i|âˆš|A|âˆšT and weighted average regret for the player is at most 2âˆ† |I i|âˆš|A|/âˆšT .Proof. The weight of iteration t < T is wt = Î  T âˆ’1

> i=ti3/2
> i3/2+1

and wT = 1. Thus, wt â‰¤ 1 for all t and therefore âˆ‘Tt=1 w2

> t

â‰¤ T .Additionally, wt â‰¥ Î T âˆ’1

> i=tii+1

= tT for t < T and wT =1. Thus, âˆ‘Tt=1 wt â‰¥ T (T + 1) /(2 T ) > T / 2.Applying (6) and Lemma 3, we see that Qw,T i (I, a ) â‰¤

> âˆ†

âˆš|A|âˆšâˆ‘Tt=1 w2

> t
> âˆ‘Tt=1 wt

â‰¤ 2âˆ† âˆš|A|âˆšTT . From (4) we see that

Qw,T i â‰¤ 2âˆ† |I i|âˆš|A|âˆšTT . Since Rw,T i â‰¤ Qw,T i , so Rw,T i â‰¤

> 2âˆ† |I i|

âˆš|A|âˆšTT .Correctness of DCFR(3/2, 1/2, 2)

Theorem 3. Assume that T iterations of DCFR 32 , 12 ,2

are conducted in a two-player zero-sum game. Then the weighted average strategy profile is a 9âˆ† |I|| âˆš|A|/âˆšT -Nash equilibrium. Proof. From Lemma 5, we know that regret in DCFR 32 , 12 ,2

for any infoset I and action a cannot be below âˆ’âˆ†âˆšT .Consider the weighted sequence of iterates Ïƒâ€²1, ..., Ïƒ â€²T

in which Ïƒâ€²t is identical to Ïƒt, but weighed by wa,t =Î T âˆ’1

> i=ti2
> (i+1) 2

= 6t2

> T(T+1)(2 T+1)

rather than wt =Î T âˆ’1

> i=ti3/2
> i3/2+1

. The regret of action a in infoset I on iteration

t of this new sequence is Râ€²t(I, a ).From Lemma 4 we know that Rt(I, a ) â‰¤ 2âˆ† âˆš|A|âˆšT

for player i for action a in infoset I. Since wa,t is an in-creasing sequence, so we can apply Lemma 1 using weight

wa,t for iteration t with B = 2âˆ† âˆš|A|âˆšT and C =

âˆ’âˆ†âˆš|A|âˆšT . From Lemma 1, this means that Râ€²t(I, a ) â‰¤

> 6T2(3âˆ†

âˆš|A|âˆšT )

(T (T +1)(2 T +1)

) â‰¤ 9âˆ†( |âˆš|A|)/âˆšT . Applying (4), we get weighted regret is at most 9âˆ† |I i|(|âˆš|A|)/âˆšT . Since the weights sum to one, this is also weighted average regret. Since |I 1| + |I 2| = |I| , so the weighted average strategies form a 9âˆ†( |I| (âˆš|A|)/âˆšT -Nash equilibrium.

Lemma 5. Suppose after each of T iterations of CFR, re-gret is multiplied by âˆštâˆšt+1 on iteration t. Then RT (I, a ) â‰¥âˆ’âˆ†âˆšT for any infoset I and action a.Proof. We prove this inductively. On the first iteration, the lowest regret could be after multiplying by âˆš1âˆš1+1 is âˆ’ âˆ†2 .Now assume that after T iterations of CFR in which regret is multiplied by âˆštâˆšt+1 on each iteration, RT (I, a ) â‰¥ âˆ’ âˆ†âˆšT

for infoset I action a. After conducting an additional iter-ation of CFR and multiplying by âˆšT +1 âˆšT +1+1 , RT +1 (I, a ) â‰¤âˆ’âˆ†( âˆšT + 1) âˆšT +1 âˆšT +1+1 . Since âˆšT + 1 â‰¤ âˆšT + 1 + 1, so âˆ’ âˆšT +1 âˆšT +1+1 âˆ†âˆšT + 1 = âˆ’âˆ†( âˆšT + 1) âˆšT +1 âˆšT +1+1 â‰¥âˆ’âˆ†âˆšT + 1 . Thus, RT +1 (I, a ) â‰¥ âˆ’ âˆ†âˆšT + 1 .
