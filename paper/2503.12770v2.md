Title: Faster Game Solving via Asymmetry of Step Sizes

URL Source: https://arxiv.org/pdf/2503.12770

Published Time: Fri, 14 Nov 2025 01:20:27 GMT

Number of Pages: 22

Markdown Content:
# Faster Game Solving via Asymmetry of Step Sizes

Linjian Meng, 1 Tianpei Yang, 1* Youzhi Zhang, 2âˆ— Zhenxing Ge, 1 Yang Gao 1

> 1

National Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China

> 2

Centre for Artificial Intelligence and Robotics, Hong Kong Institute of Science & Innovation, CAS menglinjian@smail.nju.edu.cn, tianpei.yang@nju.edu.cn, youzhi.zhang@cair-cas.org.hk, zhenxingge@smail.nju.edu.cn, gaoy@nju.edu.cn

Abstract

Counterfactual Regret Minimization (CFR) algorithms are widely used to compute a Nash equilibrium (NE) in two-player zero-sum imperfect-information extensive-form games (IIGs). Among them, Predictive CFR + (PCFR +) is partic-ularly powerful, achieving an exceptionally fast empirical convergence rate via the prediction in many games. However, the empirical convergence rate of PCFR + would significantly degrade if the prediction is inaccurate, leading to unstable performance on certain IIGs. To enhance the robustness of PCFR +, we propose Asymmetric PCFR + (APCFR +), which employs an adaptive asymmetry of step sizes between the updates of implicit and explicit accumulated counterfactual regrets to mitigate the impact of the prediction inaccuracy on convergence. We present a theoretical analysis demonstrat-ing why APCFR + can enhance the robustness. To the best of our knowledge, we are the first to propose the asymmetry of step sizes, a simple yet novel technique that effectively improves the robustness of PCFR +. Then, to reduce the diffi-culty of implementing APCFR + caused by the adaptive asym-metry, we propose a simplified version of APCFR + called Simple APCFR + (SAPCFR +), which uses a fixed asymme-try of step sizes to enable only a single-line modification compared to original PCFR +. Experimental results on five standard IIG benchmarks and two heads-up no-limit Texas Holdâ€™em (HUNL) Subagems show that (i) both APCFR + and SAPCFR + outperform PCFR + in most of the tested games, (ii) SAPCFR + achieves a comparable empirical convergence rate with APCFR +, and (iii) our approach can be generalized to improve other CFR algorithms, e.g. , Discount CFR (DCFR).

Code â€”https://github.com/menglinjian/AAAI-2026-APCFRPlus

1 Introduction

Imperfect-information extensive-form games (IIGs) are foun-dational models to capture interactions among multiple agents in sequential settings with hidden information. IIGs are widely used to simulate real-world scenarios such as medical treatment (Sandholm 2015), security games (Lis `y, Davis, and Bowling 2016), cybersecurity (Chen et al. 2017), and recreational games (Brown and Sandholm 2018, 2019b).

> *

Corresponding Authors. Copyright Â© 2026, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. Game

> Game
> ğ‘¹ !
> "#$
> ğ‘¹ !
> "
> ğ‘¹ !
> "%$
> "ğ‘¹ !
> "#$
> "ğ‘¹ !
> "
> ğ’“ !
> "#$
> ğ’“ !
> "
> prediction
> prediction
> ğ’“ !
> "#&
> ğ’“ !
> "#$
> ğ’“ !
> "#$
> Game "ğ‘¹ !
> "%$prediction
> ğ’“ !
> "
> ğ’“ !
> "
> ğ’“ !
> "%$
> ğœ "#$
> ğœ "
> ğœ "%$
> ğ’“ !
> "#&
> 1+ğ›¼ !
> "#$
> Game
> Game
> ğ‘¹ !
> "#$
> ğ‘¹ !
> "
> ğ‘¹ !
> "%$
> "ğ‘¹ !
> "#$
> "ğ‘¹ !
> "
> ğ’“ !
> "#$
> ğ’“ !
> "
> prediction
> prediction
> ğ’“ !
> "#$
> Game "ğ‘¹ !
> "%$prediction
> ğ’“ !
> "
> ğ’“ !
> "%$
> ğœ "#$
> ğœ "
> ğœ "%$
> (a) The update rule of PCFR +(b) The update rule of APCFR +
> ğ’“ !
> "#$
> 1+ğ›¼ !
> "
> ğ’“ !
> "
> 1+ğ›¼ !
> "%$

Figure 1: Comparison between PCFR + and APCFR +, with differences highlighted in red. Note that the notation t in Î±tI

denotes iteration t, rather than an exponent. To address IIGs, a primary goal is to compute a Nash equi-librium (NE), where no player can unilaterally improve its payoff by deviating from the equilibrium. As with much of the literature on solving IIGs, we focus on computing an NE in two-player zero-sum IIGs. The most widely used method for computing an NE in these IIGs is Counterfactual Regret Minimization (CFR) (Zinkevich et al. 2007; Lanctot et al. 2009; Tammelin 2014; Brown and Sand-holm 2019a; Farina, Kroer, and Sandholm 2021, 2019; Liu et al. 2021, 2023; Meng et al. 2023; Farina et al. 2023; Xu et al. 2022, 2024b,a; Zhang, McAleer, and Sandholm 2024), as evidenced by their success in superhuman game AIs (Bowl-ing et al. 2015; Morav Ë‡cÂ´Ä±k et al. 2017; Brown and Sandholm 2018, 2019b; P Â´erolat et al. 2022). The key insight of CFR algorithms is to decompose the total regret over the game into a sum of counterfactual regrets associated within infor-mation sets (infosets) and employ a local regret minimizer to minimize counterfactual regrets within each infoset. Many technologies have been proposed to improve the empirical convergence rate of CFR algorithms. For example, Counterfactual Regret Minimization + (CFR +) (Tammelin 2014) replaces the local regret minimizerâ€”Regret Matching (RM) (Hart and Mas-Colell 2000; Gordon 2006)â€”used in vanilla CFR with Regret Matching + (RM +). CFR + improves the empirical convergence rate by ensuring that the accumu-lated counterfactual regrets remain non-negative. Then, Fa-rina, Kroer, and Sandholm (2021) introduce Predictive CFR +

(PCFR +), an improved variant of CFR +.

> arXiv:2503.12770v2 [cs.LG] 13 Nov 2025 02000 4000
> Iterations
> 10 5
> 10 4
> 10 3
> 10 2
> 10 1
> 10 0
> Normalized Value
> Leduc Poker
> 02000 4000
> Iterations
> 10 5
> 10 4
> 10 3
> 10 2
> 10 1
> 10 0
> Battleship (3,2,3)

Figure 2: Dynamics of inaccuracy in PCFR + between pre-dicted and observed instantaneous counterfactual regrets in Leduc Poker and Battleship (3,2,3). This inaccuracy is related to the theoretical convergence rate of PCFR +. The values on the y-axis are normalized to the range [0, 1], which is displayed on a logarithmic scale. PCFR + significantly outperforms other CFR algorithms including CFR + in many IIGs by using the prediction. Specif-ically, PCFR + maintains two types of accumulated counter-factual regrets: the implicit and the explicit. As shown in Figure 1, at each iteration t, PCFR + uses the prediction and the observed instantaneous counterfactual regret rtI to derive the new explicit accumulated counterfactual regret Ë†RtI and the new implicit counterfactual regret Rt+1

> I

, respectively. If the prediction aligns with the observed instantaneous counter-factual regret rtI , the theoretical convergence rate of PCFR +

can be improved from O(1 /âˆšT ) of CFR + to O(1 /T ) (Fa-rina, Kroer, and Sandholm 2021). However, PCFR + sets the instantaneous counterfactual regret rtâˆ’1

> I

observed at itera-tion t âˆ’ 1 as the prediction at iteration t. This operation may cause inaccurate prediction on certain IIGs, which harms the empirical convergence rate of PCFR +. As noted by Farina, Kroer, and Sandholm (2021), PCFR + underperforms other CFR algorithms in Leduc Poker (Game [ O] in Farina, Kroer, and Sandholm (2021)), yet significantly surpasses them in Battleship (3,2,3) (Game [ R] in Farina, Kroer, and Sandholm (2021)). This aligns with the results in Figure 2: the gap be-tween predicted and observed instantaneous counterfactual regret decreases slowly in Leduc Poker but diminishes rapidly in Battleship (3,2,3), validating our hypothesis. To enhance the robustness of PCFR +, we propose a novel variant of PCFR +, termed Asymmetric PCFR + (APCFR +). Similar to PCFR +, APCFR + leverages the prediction to im-prove the convergence rate, but it mitigates the impact of the prediction inaccuracy on convergence. Specifically, as illus-trated in Figure 1, APCFR + utilizes an adaptive asymmetry mechanism for step sizes between implicit and explicit ac-cumulated counterfactual regret updates, which dynamically reduces the step size when updating via the prediction. We prove that when the step size for updating the explicit accu-mulated counterfactual regret via the prediction at iteration t

is set to 1/(1 + Î±t) for APCFR +, where Î±t â‰¥ 0 is a constant, the effect of the prediction inaccuracy on the convergence rate for APCFR + is reduced by a factor of 1 + Î±t compared to PCFR +. Therefore, APCFR + mitigates the impact of the prediction inaccuracy on the convergence rate. Then, through the theoretical analysis of APCFR +, we propose an auto-matic learning mechanism for Î±t, eliminating the need for fine-tuning parameters across different games. To the best of our knowledge, we are the first to propose the asymme-try of step sizes updating implicit and explicit accumulated counterfactual regrets. To simplify the implementation of APCFR + caused by the automatic learning approach of Î±t, we introduce a simplified version of APCFR +, called Simple APCFR + (SAPCFR +). Specifically, by analyzing the upper bounds of different terms within the theoretical guarantee of APCFR + (detailed at the beginning of Section 4.2), SAPCFR + sets Î±t to 2, ensuring SAPCFR + requires only a single-line modification to the original PCFR + code. We conduct extensive experimental evaluations of APCFR + and SAPCFR + across five standard IIG bench-marks as well as two heads-up no-limit Texas Holdâ€™em (HUNL) Subgames generated by the top poker agent, Libra-tus (Brown and Sandholm 2018). The experiments demon-strate that APCFR + and SAPCFR + outperforms PCFR + in nearly all tested games and achieve an empirical convergence rate comparable to that of PCFR + in the remaining games. Moreover, we observe that SAPCFR + achieves comparable empirical convergence rate with APCFR +. Finally, we can observe that our approach can be generalized to improve other CFR algorithms, e.g. , Discount CFR (DCFR) (Brown and Sandholm 2019a).

2 Related Work

We consider CFR algorithms (Zinkevich et al. 2007; Tam-melin 2014; Brown and Sandholm 2019a; Farina, Kroer, and Sandholm 2021, 2019; Liu et al. 2021; P Â´erolat et al. 2021; Liu et al. 2023; Meng et al. 2023; Farina et al. 2023; Xu et al. 2022, 2024b,a; Zhang, McAleer, and Sandholm 2024), the most widely used method for learning an NE in two-player zero-sum IIGs, as evidenced by their success in superhuman game AIs (Bowling et al. 2015; Morav Ë‡cÂ´Ä±k et al. 2017; Brown and Sandholm 2018, 2019b; PÂ´ erolat et al. 2022). The key insight of CFR algorithms is the decomposition of the regret over the game into the sum of counterfactual regrets associated with infosets. The vanilla CFR algorithm, introduced by Zinkevich et al. (2007), employs RM (Hart and Mas-Colell 2000) as the local regret minimizer. To improve the empirical convergence rate of CFR, it is common to de-sign more effective local regret minimizers, as the selection of the local regret minimizers has a significant impact on the overall performance of the CFR algorithm. Examples include RM + (Bowling et al. 2015), Discounted RM (DRM) (Brown and Sandholm 2019a), and PRM + (Farina, Kroer, and Sand-holm 2021), which correspond to CFR +, DCFR, and PCFR +,respectively. PCFR + can demonstrate an extremely faster em-pirical convergence rate than other CFR variants. However, as shown in its original paper, PCFR + is outperformed by CFR + and DCFR even on standard IIG benchmarks like Leduc Poker. To improve the robustness of PCFR +, Farina et al. (2023) propose Stable PCFR + and Smooth PCFR +. These algo-rithms improve the robustness by addressing the instability,

i.e. , rapid strategy fluctuations across iterations, via ensuring the lower bound of the 1-norm of accumulated counterfactual regrets exceeds a positive constant. However, these algo-rithms never outperform PCFR + in terms of the empirical convergence rate even though they achieve a faster theoretical convergence rate than PCFR +, as demonstrated in our experi-ments. APCFR + does not focus on addressing the instability, but instead aims to mitigate the impact of the prediction in-accuracy on the convergence to improve the robustness. In our experiments, APCFR + consistently outperforms Stable PCFR + and Smooth PCFR + in all tested games.

3 Preliminaries

Imperfect-information Extensive-form games (IIGs). To model tree-form sequential decision-making problems with hidden information, a common used model is IIG (Os-borne et al. 2004). An IIG can be formulated as G =

{N , H, P, A, I, {ui}} . Here, N = {0, 1} is the set of play-ers. â€œNatureâ€ is also considered a player c (representing chance) and chooses actions with a fixed known probability distribution. H is the set of all possible histories. For each history h âˆˆ H , the function P (h) represents the player acting at history h, and A(h) denotes the actions available at history

h. To account for private information, the histories for each player i are partitioned into a collection Ii, referred to as information sets (infosets). For any infoset I âˆˆ I i, histories

h, h â€² âˆˆ I are indistinguishable to player i. The notation I

denotes I = {I i|i âˆˆ N } . Thus, we have P (I) = P (h),

A(I) = A(h), âˆ€h âˆˆ I. The set of leaf nodes is denoted by Z.For each leaf node z, there is a pair (u0(z), u 1(z)) âˆˆ [âˆ’1, 1]

which denotes the payoffs for the min player (player 0) and the max player (player 1) respectively. In two-player zero-sum IIGs, u0(z) = âˆ’u1(z), âˆ€z âˆˆ Z .

Behavioral strategy. This strategy Ïƒi is defined on each in-foset. For any infoset I âˆˆ I i, the probability for an action a âˆˆ

A(I) is denoted by Ïƒi(I, a ). We use Ïƒi(I) = [ Ïƒi(I, a )|a âˆˆ

A(I)] âˆˆ âˆ†|A(I)| to denote the strategy at infoset I, where

âˆ†|A(I)| is a (|A(I)| âˆ’ 1) -dimension simplex. If every player follows the strategy profile Ïƒ = [ Ïƒ0; Ïƒ1] and reaches infoset

I, the reaching probability is denoted by Ï€Ïƒ (I). The prob-ability contribution of player i is Ï€Ïƒi (I), while for players other than i, denoted as âˆ’i, the contribution is Ï€Ïƒ

> âˆ’i

(I). In IIGs, ui(Ïƒ) = ui(Ïƒi, Ïƒ âˆ’i) = P

> zâˆˆZ

ui(z)Ï€Ïƒ (z).

Nash equilibrium (NE). NE denotes a rational behavior where no player can benefit by unilaterally deviating from the equilibrium. For any player, her strategy is the best-response to the strategies of others. Formally, for any NE strategy pro-file Ïƒâˆ— and i âˆˆ N , it holds that ui(Ïƒâˆ—

> i

, Ïƒ âˆ—âˆ’i) â‰¥ ui(Ïƒi, Ïƒ âˆ—âˆ’i)

for all Ïƒ. A widely used metric to measure the distance from the given strategy profile x to NE is the exploitabil-ity, which is defined as Ïµ(Ïƒ) = P

> iâˆˆN

max Ïƒâ€²

> i

(ui(Ïƒâ€²

> i

, Ïƒ âˆ’i) âˆ’

ui(Ïƒi, Ïƒ âˆ’i)) /|N | .

Computing an NE via regret minimization algorithms.

To compute an NE in IIGs, a common used method is regret minimization algorithms (Rakhlin and Sridharan 2013a,b; Hazan et al. 2016; Joulani, Gy Â¨orgy, and Szepesv Â´ari 2017). For any sequence of strategies Ïƒ1

> i

, Â· Â· Â· , Ïƒ Ti of player

i, player iâ€™s regret is RTi = max Ïƒi

PTt=1 ui(Ïƒi, Ïƒ t

> âˆ’i

) âˆ’

PTt=1 ui(Ïƒti , Ïƒ t

> âˆ’i

). Regret minimization algorithms are algo-rithms ensuring RTi grows sublinearly. If each player follows a regret minimization algorithm, then their average strategy converges to the set of the NE in two-player zero-sum IIGs. Formally, assume the regret of each player i is RTi , then it holds that

Ïµ(Â¯ Ïƒ) = Ïµ(Â¯ Ïƒ0, Â¯Ïƒ1) â‰¤ X

> iâˆˆN

RTi /(|N | T ),

where Â¯Ïƒi(I) = PTt=1 Ï€Ïƒt

> i

(I)Ïƒti (I)/ PTt=1 Ï€Ïƒt

> i

(I).

Counterfactual Regret Minimization (CFR) frame-work. This framework (Zinkevich et al. 2007; Farina, Kroer, and Sandholm 2019; Liu et al. 2021) is designed to compute an NE of two-player zero-sum IIGs. Instead of directly minimizing the global regret RTi , it decom-poses the regret to each infoset and independently min-imizes the local regret within each infoset. Let Ïƒt be the strategy profile at iteration t. This framework com-putes the counterfactual value at infoset I for action a

as vÏƒt

(I, a ) = P

> hâˆˆI

P

> zâˆˆZ ha

Ï€Ïƒt

> âˆ’i

(h)Ï€Ïƒt

(ha, z )ui(z),

where Ï€Ïƒt

(ha, z ) denotes the probability from ha to z

if all players play according to Ïƒt and Zha is the set of the leaf nodes that are reachable after choosing ac-tion a at history h. For any infoset I, the counterfac-tual regret is RT (I) = max aâˆˆA(I)

PTt=1 vÏƒt

(I, a ) âˆ’

PTt=1

P

> aâ€²âˆˆA(I)

Ïƒti (I, a â€²)vÏƒt

(I, a â€²). The regret over the game RTi = max Ïƒi

PTt=1 ui(Ïƒi, Ïƒ t

> âˆ’i

)âˆ’PTt=1 ui(Ïƒti , Ïƒ t

> âˆ’i

) is less than the sum of the counterfactual regrets within infosets:

RTi â‰¤ P

> IâˆˆI i

RT (I). So any regret minimization algorithms can be used as the local regret minimizer to minimize the regret RT (I) over each infoset to minimize the regret RTi .

Predictive Counterfactual Regret Minimization +

(PCFR +). PCFR + (Farina, Kroer, and Sandholm 2021) is a powerful CFR algorithm, which significantly outperforms other CFR algorithm in many IIGs. PCFR + employs Predic-tive RM + (PRM +) (Farina, Kroer, and Sandholm 2021) as its local regret minimizer, with its key insight is to use the prediction. Specifically, as shown Figure 1, at each iteration

t, PCFR + maintains implicit and explicit accumulated coun-terfactual regrets: RtI and Ë†Rtâˆ’1

> I

. Firstly, PCFR + makes a prediction and uses this prediction to derive new explicit ac-cumulated counterfactual regrets Ë†RtI from RtI . Then, PCFR +

observes the instantaneous counterfactual regret rtI by follow-ing the strategy Ïƒt defined by Ë†RtI . Lastly, rtI is subsequently used to derive Rt+1

> I

from RtI . If the prediction aligns with the observed instantaneous counterfactual regret rtI , Farina, Kroer, and Sandholm (2021) show that the theoretical conver-gence of PCFR + can be improved from O(1 /âˆšT ) of CFR +

to O(1 /T ). As tested in Farina, Kroer, and Sandholm (2021), using the instantaneous counterfactual regret rtâˆ’1

> I

observed at the previous iteration t âˆ’ 1 as the prediction is both simple and effective. Therefore, in practice, PCFR + uses rtâˆ’1

> I

as the prediction at iteration t. Formally, at each iteration t and for each infoset I âˆˆ I , PCFR + updates its strategy according to

Ë†RtI = [ RtI + rtâˆ’1

> I

]+, Rt+1

> I

= [ RtI + rtI ]+,Ïƒti (I) = [ Ë†RtI ]+

âˆ¥[ Ë†RtI ]+âˆ¥1

= Ë†RtI

âˆ¥ Ë†RtI âˆ¥1

,where i = P (I), R1

> I

= 0, and the forth equality comes from the fact that Ë†RtI â‰¥ 0.

4 Methodology

PCFR + leverages the prediction to accelerate the empirical convergence rate. However, when the prediction is inaccurate, its empirical convergence rate may decrease significantly, leading to unstable performance on certain IIGs. To enhance the robustness of PCFR +, we propose Asymmetric PCFR +

(APCFR +), which mitigates the impact of the prediction inac-curacy on the convergence rate via the adaptive asymmetry of step sizes. We then provide a theoretical analysis for APCFR +

to demonstrate the reason why it enhances the robustness. To simplify the implementation of APCFR + due to the adaptive asymmetry, we propose Simple APCFR + (SAPCFR +), using a constant asymmetry to guarantee that it can be implemented with a single-line modification compared to PCFR +.

4.1 Asymmetric PCFR + (APCFR +)

To mitigate the impact of the prediction inaccuracy on con-vergence of PCFR +, APCFR + adaptively reduces the step size when updating via the prediction, i.e., when updating the explicitly accumulated counterfactual regret. In other words, APCFR + exploits the adaptive asymmetry of step sizes be-tween the updates of the implicit and explicit ones. Formally, at iteration t and infoset I, the update rule of APCFR + is

Ë†RtI = [ RtI + 11 + Î±tI

rtâˆ’1

> I

]+, Rt+1

> I

= [ RtI + rtI ]+,Ïƒti (I) = [ Ë†RtI ]+

âˆ¥[ Ë†RtI ]+âˆ¥1

= Ë†RtI

âˆ¥ Ë†RtI âˆ¥1

,

where i = P (I), R1

> I

= 0, and r0

> I

= 0. The comparison between the update rules of PCFR + and APCFR + has been shown in Figure 1. In the rest of this subsection, we first present the regret upper bound for APCFR + with respect to any Î±tI , as stated in Theorem 4.1. According to the discussion about Theorem 4.1, we show why APCFR + can enhance the robustness of PCFR + by mitigating the impact of the prediction inaccuracy on the convergence rate. Lastly, we discuss how to automatically learn Î±tI from the regret bound shown in Theorem 4.1.

Theorem 4.1. [Proof is in Appendix A]. Assuming that T

iterations of APCFR + with any Î±tI â‰¥ 0 are conducted, the counterfactual regret at any infoset I âˆˆ I is bound by

RT (I) â‰¤

vuut TX

> t=1

 âˆ¥rtI âˆ’ rtâˆ’1

> I

âˆ¥22

1 + Î±tI

+ Î±tI âˆ¥Rt+1

> I

âˆ’ RtI âˆ¥22



.

Why the asymmetry mechanism is effective. To assess the effectiveness of the asymmetry mechanism for step sizes in decreasing the regret upper bound (improving the conver-gence rate), we show the upper bound of âˆ¥rtI âˆ’ rtâˆ’1

> I

âˆ¥22 is four times than that of âˆ¥Rt+1

> I

âˆ’ RtI âˆ¥22. Firstly, we introduce Lemma 4.2.

Lemma 4.2. [Adapted from Lemma 11 of Wei et al. (2021)]. Assume that T iterations of APCFR + with any Î±tI â‰¥ 0 are conducted. Then for any infoset I âˆˆ I and t â‰¥ 1, we have

âˆ¥Rt+1

> I

âˆ’ RtI âˆ¥22 â‰¤ âˆ¥ rtI âˆ¥22.

Assume that for any infoset I âˆˆ I and t â‰¥ 1, âˆ¥rtI âˆ¥22 â‰¤ E.Then, from Lemma 4.2, we have

âˆ¥Rt+1

> I

âˆ’ RtI âˆ¥22 â‰¤ E. (1) Similarly, for âˆ¥rtI âˆ’ rtâˆ’1

> I

âˆ¥22, we have

âˆ¥rtI âˆ’ rtâˆ’1

> I

âˆ¥22 â‰¤ 4E. (2) In experiments, we also analyze the values of two terms

PTt=1 âˆ¥rtI âˆ’ rtâˆ’1

> I

âˆ¥22 and PTt=1 âˆ¥Rt+1

> I

âˆ’ RtI âˆ¥22, for both PCFR + and our algorithms (Figures 6 and 7). Among all al-gorithms, we observe that the value of PTt=1 âˆ¥rtI âˆ’ rtâˆ’1

> I

âˆ¥22 is at least three times than that of PTt=1 âˆ¥Rt+1

> I

âˆ’RtI âˆ¥22. This in-dicates that introducing the term PTt=1 Î±tI âˆ¥Rt+1

> I

âˆ’RtI âˆ¥22 and modifying the term PTt=1 âˆ¥rtI âˆ’rtâˆ’1

> I

âˆ¥22 to PTt=1

> âˆ¥rtIâˆ’rtâˆ’1
> Iâˆ¥22
> 1+ Î±tI

,can reduce the regret upper bound. Furthermore, compared to PCFR +, both of these two terms are smaller in our algorithms, further decreasing the regret upper bound. Then, we evaluate the values of PTt=1 ( âˆ¥rtI âˆ’rtâˆ’1

> Iâˆ¥22
> 1+ Î±tI

+ Î±tI âˆ¥Rt+1

> I

âˆ’ RtI âˆ¥22), for both PCFR + and our algorithms (Figures 8 and 9). In all games, the value of PTt=1 ( âˆ¥rtI âˆ’rtâˆ’1

> Iâˆ¥22
> 1+ Î±tI

+ Î±tI âˆ¥Rt+1

> I

âˆ’ RtI âˆ¥22)

is consistently smaller in our algorithms than in PCFR +. See more details in Appendix D.

An alternative regret upper bound of APCFR +. No-tably, Theorem 4.1 does not conflict the upper regret bound of CFR + (where Î±tI â†’ âˆ ), as it provides a larger upper regret bound than the original CFR + upper bound. By altering the proof method, we get RT (I) â‰¤

qPTt=1 âˆ¥rtI âˆ’ 11+ Î±tI

rtâˆ’1

> I

âˆ¥22,as shown in Theorem B.1 (detailed in Appendix B). By set-ting Î±tI â†’ âˆ , the original bound of CFR + can be recovered. Additionally, for PCFR + (where Î±tI â†’ 0), the bound in The-orem 4.1 is identical to the one in its original version (the result in Theorem 3 of the original PCFR + version can be easily improved to the bound presented in Theorem 4.1). The reason why we employ Theorem 4.1 in the main text rather than Theorem B.1 is that the regret bound in Theorem B.1 is typically larger than that in Theorem 4.1, as demonstrated in Appendix D (Figures 8, 9, 10, and 11).

Automatic learning approach for Î±tI . To eliminate the fine-tuning of Î±tI , we propose an automatic learning approach for Î±tI . From Theorem 4.1, we have

RT (I) â‰¤

vuut TX

> t=1

 âˆ¥rtI âˆ’ rtâˆ’1

> I

âˆ¥22

1 + Î±tI

+ Î±tI âˆ¥Rt+1

> I

âˆ’ RtI âˆ¥22



â‰¤

vuut TX

> t=1

 âˆ¥rtI âˆ’ rtâˆ’1

> I

âˆ¥22

Î±tI

+ Î±tI âˆ¥Rt+1

> I

âˆ’ RtI âˆ¥22



.

To minimize the right-hand side of the last inequality, we can set Î±tI =

r âˆ¥rtI âˆ’rtâˆ’1

> Iâˆ¥22
> âˆ¥Rt+1
> Iâˆ’RtIâˆ¥22

. However, this is not feasible, as we need Î±tI to compute rtI . Therefore, we adopt an alternative approach:

Î±tI = min

s Ptâˆ’1

> Ï„=1

âˆ¥rÏ„I âˆ’ rÏ„ âˆ’1

> I

âˆ¥22

Ptâˆ’1

> Ï„=1

âˆ¥RÏ„ +1

> I

âˆ’ RÏ„I âˆ¥22

, Î± max

!

. (3) Note that the parameter in Î±max in Eq. (3) is included solely to ensure that the bound in Theorem 4.1 remains finite. In this paper, we directly set it as 5 to reduce the cost of hyperpa-rameter tuning. In practice, we rarely observed Î±tI reaching 5 (Figures 4 and 5).

4.2 Simple APCFR + (SAPCFR +)

To simplify the implementation of APCFR + caused by the automatic learning approach of Î±t, we introduce SAPCFR +,which is implemented with a single-line modification to the PCFR + code. Specifically, SAPCFR + sets Î±tI = 2 . The key insight of setting Î±tI = 2 lies in the fact that the upper bound of âˆ¥Rt+1

> I

âˆ’ RtI âˆ¥22 is only a quarter of the upper bound of

âˆ¥rtI âˆ’ rtâˆ’1

> I

âˆ¥22, as shown in Eq. (1), and Eq. (2). Specifically, combining Theorem 4.1, Eq. (1), and Eq. (2), in the worst case, we obtain

RT (I) â‰¤

vuut TX

> t=1

 âˆ¥rtI âˆ’ rtâˆ’1

> I

âˆ¥22

1 + Î±tI

+ Î±tI âˆ¥Rt+1

> I

âˆ’ RtI âˆ¥22



â‰¤

vuut TX

> t=1

 4E

1 + Î±tI

+ Î±tI E



.

It is evident that when Î±tI = 0 , i.e., for PCFR +, the worst-case counterfactual regret upper bound is

RT (I) â‰¤

vuut TX

> t=1

4E.

From the facts that (i) 2 minimizes PTt=1 (4 E/Î± tI + Î±tI E)

for any positive E and (ii) PTt=1 (4 E/ (1 + Î±tI ) + Î±tI E) â‰¤

PTt=1 (4 E/Î± tI + Î±tI E), we can set Î±tI = 2 , which implies the counterfactual regret is bound by

RT (I) â‰¤

vuut TX

> t=1

 4E

1 + 2 + 2 E



=

vuut TX

> t=1

10 E

3 .

Clearly, setting Î±tI = 2 results in a lower regret upper bound than PCFR +. Therefore, for SAPCFR +, we set Î±tI = 2 for all t â‰¥ 1. Formally, at each iteration t, SAPCFR + updates its strategy at each infoset I âˆˆ I according to the following update rule:

Ë†RtI = [ RtI + 13 rtâˆ’1

> I

]+, Rt+1

> I

= [ RtI + rtI ]+,Ïƒti (I) = [ Ë†RtI ]+

âˆ¥[ Ë†RtI ]+âˆ¥1

= Ë†RtI

âˆ¥ Ë†RtI âˆ¥1

,

where i = P (I), R1

> I

= 0, and r0

> I

= 0.

5 Experiments

Configurations. We now evaluate the empirical convergence rates of APCFR + and SAPCFR + by comparing them to PCFR +, Stable PCFR +, Smooth PCFR +, Reg-CFR (Liu et al. 2023), and Clairvoyant CFR (Farina et al. 2023). Stable PCFR + and Smooth PCFR + are advanced PCFR + variants. Reg-CFR and Clairvoyant CFR achieve theoretical conver-gence rates of O(1 /T 34 ) and O(1 /T ), respectively, while that of other algorithms is O(1 /âˆšT ). Following the settings in PCFR +, we employ alternating updates for both APCFR +

and SAPCFR +. For Stable PCFR +, Smooth PCFR +, and Reg-CFR, we apply alternating updates, as described in their original paper or open-source code. Clairvoyant CFR does not utilize alternating updates, in accordance with its original design. For all algorithms, we utilize quadratic averaging. For all compared algorithms, we adopt the hyperparameters as suggested in their respective original versions. Details on the size of the tested games are in Appendix D (Table 3). The experiments are conducted on a machine equipped with a Xeon(R) Gold 6444Y CPU and 256 GB of memory.

Empirical convergence rates in standard IIG bench-marks. We now present the empirical convergence rates across five standard IIG benchmarks, e.g. , Kuhn Poker, Leduc Poker, Goofspiel Poker, Liarâ€™s Dice, and Battleship. These games are implemented using OpenSpiel (Lanctot et al. 2019). The algorithm implementations are based on LiteEFG (Liu, Farina, and Ozdaglar 2024), as LiteEFG provides approxi-mately 100 times speedup compared to the default implemen-tation in OpenSpiel. The results are in Figure 3. For most of the tested games, except for Battleship (3,2,3) and Goof-spiel (4), APCFR + and SAPCFR +, significantly outperform all baselines. Even in Battleship (3,2,3) and Goofspiel (4), APCFR + and SAPCFR + outperform all algorithms except PCFR +. Remarkably, they exhibit performance comparable to PCFR +, reaching similar levels of exploitability after 5000 iterations. Based on the experimental results in Appendix D (Figures 6 and 7), we observe that in the games where our algorithms perform similar to PCFR +, such as Battleship (3,2,3) and Goofspiel (4), PCFR + also exhibits a rapid de-crease in the inaccuracy between the predicted and observed instantaneous counterfactual regrets (detailed discussions are in Appendix D). Furthermore, the performance gap between APCFR + and SAPCFR + is relatively small. Specifically, APCFR + only outperforms SAPCFR + in Leduc Poker, Bat-tleship (3,2,3), and Liarâ€™s Dice (5). This small performance gap means that in practical applications, SAPCFR + can be directly used due to its ease of implementation and a faster empirical convergence rate compared to PCFR +. Regarding Stable PCFR + and Smooth PCFR +, we find that they sig-nificantly underperform PCFR +. Reg-CFR and Clairvoyant CFR significantly underperform relative to other algorithms.

Empirical convergence rates in HUNL Subgames. To assess the performance of APCFR + and SAPCFR + in ad-dressing real-world games, we also conduct evaluations in HUNL Subgames, which are considerably larger than stan-dard IIG benchmarks. Despite the presence of code related to HUNL Subgames in Openspiel, we have not successfully executed it. Therefore, we utilize HUNL Subgames imple-0 1000 2000 3000 4000 5000

> 10 9
> 10 7
> 10 5
> 10 3
> 10 1
> Exploitability
> kuhn Poker
> 01000 2000 3000 4000 5000
> 10 4
> 10 3
> 10 2
> 10 1
> 10 0
> Leduc Poker
> 01000 2000 3000 4000 5000
> 10 8
> 10 7
> 10 6
> 10 5
> 10 4
> 10 3
> 10 2
> 10 1
> 10 0
> Battleship (3,2,3)
> 01000 2000 3000 4000 5000
> 10 9
> 10 8
> 10 7
> 10 6
> 10 5
> 10 4
> 10 3
> 10 2
> 10 1
> Battleship (4,3,2)
> 01000 2000 3000 4000 5000
> Iterations
> 10 8
> 10 7
> 10 6
> 10 5
> 10 4
> 10 3
> 10 2
> 10 1
> 10 0
> Exploitability
> Liar's Dice (4)
> 01000 2000 3000 4000 5000
> Iterations
> 10 7
> 10 6
> 10 5
> 10 4
> 10 3
> 10 2
> 10 1
> 10 0
> Liar's Dice (5)
> 01000 2000 3000 4000 5000
> Iterations
> 10 8
> 10 6
> 10 4
> 10 2
> 10 0
> Goofspiel (4)
> 01000 2000 3000 4000 5000
> Iterations
> 10 7
> 10 6
> 10 5
> 10 4
> 10 3
> 10 2
> 10 1
> 10 0
> Goofspiel (5)
> PCFR +Stable PCFR +Smooth PCFR +Reg-CFR Clairvoyant CFR APCFR +SAPCFR +

Figure 3: Empirical convergence rates of the tested algorithms in standard commonly used IIG benchmarks. In all plots, the x-axis is the number of iterations, and the y-axis is exploitability, displayed on a logarithmic scale. Liarâ€™s Dice ( x) represents that every player is given a die with x sides. Goofspiel ( x) denotes that each player is dealt x cards. Battleship ( x, y, z ) implies the size of the grid is x Ã— y, and the number of shots is z.mented by Poker RL (Steinberger 2019). More precisely, our code is based on the code from Xu et al. (2024b). The code in Xu et al. (2024b) supports only Subgame 3 and Subgame 4, so we conduct experiments solely on these two HUNL Sub-games. We do not compare Reg-CFR and Clairvoyant CFR in HUNL Subgames, as they perform significantly worse than other CFR algorithms, even in standard IIG benchmarks. The results are shown in Table 1: APCFR+ and SAPCFR+ consistently outperform all baselines in both subgames.

Running times. To validate the efficiency of APCFR +

and SAPCFR +, we compare their running time with that of PCFR + under the same number of iterations ( i.e. , 5000). The experimental results are in Appendix D (Table 4). The running time of APCFR + is slightly higher compared to PCFR +, primarily due to the additional Î±tI learning process in APCFR +. However, the running time of SAPCFR + is nearly identical to that of PCFR +, as the only difference between their implementations is a single line of code, which does not alter the computational complexity. Notably, the computational complexity remains exactly the same, even with no change in the constant factors.

Dynamics of Î±tI in APCFR +. To study the behavior of Î±tI ,we analyze its dynamics, as shown in Appendix D (Figures 4 and 5). We observe that Î±tI experiences a rapid increase during the initial phase but ceases to grow after approximately 100 iterations. This might be due to that the values of âˆ¥rtI âˆ’

rtâˆ’1

I âˆ¥22 and âˆ¥Rt+1

I âˆ’ RtI âˆ¥22 are significantly larger in the initial phase than at later stages (as also observed in Figures 6 and 7). More details are in Appendix D.

Dynamics of PTt=1 âˆ¥rtI âˆ’ rtâˆ’1

I âˆ¥22, PTt=1 âˆ¥Rt+1

I âˆ’ RtI âˆ¥22,

PTt=1 ( âˆ¥rtI âˆ’rtâˆ’1

> I

âˆ¥22

1+ Î±tI

+ Î±tI âˆ¥Rt+1

I âˆ’ RtI âˆ¥22), and PTt=1 âˆ¥rtI âˆ’

rtâˆ’1

> I

1+ Î±tI

âˆ¥22. To evaluate the regret bound presented in our the-oretical analysis, we examine the dynamics of these terms, as demonstrated in Appendix D. Specifically, the dynamics of PTt=1 âˆ¥rtI âˆ’ rtâˆ’1

I âˆ¥22 and PTt=1 âˆ¥Rt+1

I âˆ’ RtI âˆ¥22 are pre-sented in Figures 6 and 7. Similarly, Figures 8 and 9 present the dynamics of PTt=1

 âˆ¥rtI âˆ’rtâˆ’1

> I

âˆ¥22

1+ Î±tI

+ Î±tI âˆ¥Rt+1

I âˆ’ RtI âˆ¥22



.Additionally, Figures 10 and 11 depict the dynamics of

PTt=1 âˆ¥rtI âˆ’ rtâˆ’1

> I

1+ Î±tI

âˆ¥22. This experimental results show that

âˆ¥rtI âˆ’ rtâˆ’1

I âˆ¥22 are larger than âˆ¥Rt+1

I âˆ’ RtI âˆ¥22, which confirms that APCFR + effectively reduces the impact of âˆ¥Rt+1

I âˆ’

RtI âˆ¥22 by increasing the weights on âˆ¥Rt+1

I âˆ’ RtI âˆ¥22. For the first three terms, their values are smaller in APCFR + and SAPCFR + compared to PCFR +, implying a lower regret bound in Theorem 4.1. However, the value of PTt=1 âˆ¥rtI âˆ’

rtâˆ’1

> I

1+ Î±tI

âˆ¥22 significantly exceeds that of PTt=1 ( âˆ¥rtI âˆ’rtâˆ’1

> I

âˆ¥22

1+ Î±tI

+

Î±tI âˆ¥Rt+1

I âˆ’ RtI âˆ¥22), indicating that the regret bound in Theo-PCFR + Stable PCFR + Smooth PCFR + APCFR + SAPCFR +

Subgame 3 1.44e-3 1.41e-3 (-2.1%) 1.42e-3 (-1.4%) 1.02e-3 (-29.2%) 9.44e-4 (-34.4%) Subgame 4 1.04e-3 9.77e-4 (-5.3%) 1.02e-3 (-1.9%) 7.53e-4 (-27.6%) 7.83e-4 (-24.7%) Table 1: Final exploitability for the tested algorithms in HUNL Subgames. Values in red indicate percentages relative to PCFR +.Leduc Poker (5) Leduc Poker (9) Leduc Poker (13) DCFR 2.79e-5 1.27e-5 1.09e-5

PCFR + 2.69e-5 5.21e-5 3.15e-5

APCFR + 4.80e-6 (-82.1%) 4.03e-5 (-22.6%) 1.45e-5 (-54.0%) SAPCFR + 3.49e-6 (-87.0%) 4.07e-5 (-21.9%) 1.42e-5 (-55.0%) DCFR + 1.15e-5 (-58.8%) 6.41e-6 (-49.5%) 8.56e-6 (-21.6%) APDCFR + 3.69e-6 (-86.3%, -86.7%) 3.42e-6 (-93.4%, -73.1%) 3.02e-6 (-90.4%, -72.3%) Table 2: The final exploitability for DCFR, PCFR +, APCFR +, SAPCFR +, DCFR +, and APDCFR + in Leduc Poker variants. Values in red indicate percentages of PCFR + variants relative to PCFR +, and values in blue indicate percentages of DCFR variants relative to DCFR. Notably, APDCFR + can serve as a variant of both PCFR + and DCFR. rem B.1 is extremely higher than that in Theorem 4.1. Thus, we use Theorem 4.1 in the main text instead of Theorem B.1.

Empirical convergence rates of APCFR + with an alter-native learning approach for Î±t. We also experiment with a different learning approach for Î±t, other than the one in Eq. (3), e.g. , Î±tI = min

r max Ï„ âˆˆ[tâˆ’1] âˆ¥rÏ„I âˆ’rÏ„ âˆ’1

> Iâˆ¥22
> max Ï„âˆˆ[tâˆ’1] âˆ¥RÏ„+1
> Iâˆ’RÏ„Iâˆ¥22

, Î± max



,where we also set Î±max = 5 as did in Eq. (3) to reduce the cost of hyperparameter tuning. The results in Appendix D (Figure 12 and Table 5) indicate that this approach performs similarly to the one in Eq. (3).

Comparison with other classical CFR algorithms and the generalization of our approach. In addition to the CFR algorithms that have already been compared, we also compare APCFR + and SAPCFR + with the classic CFR algorithms: CFR, CFR +, and DCFR. Initially, we conducted experiments using standard IIG benchmarks and HUNL Subgames (Fig-ure 13 and Table 6), where APCFR + and SAPCFR + con-sistently outperformed CFR and CFR + across all games. However, in poker games like Leduc Poker and HUNL Sub-games, APCFR + and SAPCFR + did not surpass DCFR. No-tably, our algorithms and DCFR are not mutually exclusive and can be combined effectively. The core innovation of our algorithmsâ€”the asymmetry of step sizesâ€”can be inte-grated with DCFR, which involves discounting prior itera-tions when calculating accumulated regrets. Therefore, we propose APDCFR + by combining APCFR + with DCFR (de-tails of APDCFR + are in Appendix C). In addition to CFR, CFR +, DCFR, APCFR +, and SAPCFR +, we also compare APDCFR + with DCFR + (Xu et al. 2024b), which is an ad-vanced variant of DCFR. Experimental results, detailed in Appendix D (Table 6), demonstrate that APDCFR + achieves a substantially faster empirical convergence rate compared to the other evaluated algorithms. Additionally, to further evaluate the performance of DCFR, PCFR +, APCFR +, SAPCFR +, DCFR +, and APDCFR + in poker games, we conduct tests on various Leduc Poker vari-ants that are used in the original PCFR + paper (Farina, Kroer, and Sandholm 2021). Specifically, we test on Leduc Poker with ranks of 5, 9, or 13. We denote these Leduc Poker variants as Leduc Poker ( x), where x represents the num-ber of ranks, noting that the original Leduc Poker has 3 ranks. The results, in Table 2, demonstrate that APCFR + and SAPCFR + consistently outperform PCFR + across all Leduc Poker variants. Notably, the degree to which APCFR + and SAPCFR + surpass PCFR + does not depend on the size of the game. Specifically, the smallest improvement of APCFR +

and SAPCFR + over PCFR + occurs in Leduc Poker (9), where the reduction in exploitability is less than half of the re-duction observed in Leduc Poker (13). Moreover, the results indicate that DCFR does not consistently outperform PCFR +.For instance, in Leduc Poker (5), the performance of DCFR is inferior to that of PCFR +. More importantly, APDCFR +

consistently outperforms all other algorithms across each Leduc Poker variant tested, except in Leduc Poker (5), where it slightly underperforms compared to SAPCFR +.

6 Conclusions

We propose a novel variant of PCFR + called APCFR +,which employs the adaptive asymmetry of step sizes in the updates of implicit and explicit accumulated counterfactual regrets to improve the robustness of PCFR +. We also intro-duce SAPCFR +, requiring only a single line modification to PCFR +. Experimental results validate that APCFR + and SAPCFR + exhibit a faster empirical convergence rate than PCFR +. To our knowledge, we are the first to propose the asymmetry of step sizes in the updates of implicit and ex-plicit accumulated counterfactual regrets, a simple yet novel technique that effectively improves the robustness of PCFR +.Moreover, the techniques used in other CFR + algorithms are compatible with our algorithm, which shows the gener-alization of our approach. For example, for DCFR, by using our approach, we propose APDCFR +, which significantly outperforms DCFR in poker games. Future work involves designing more effective Î±t learning approaches to further enhance the empirical convergence rate. Acknowledgements

This work is supported in part by the National Natural Science Foundation of China under Grants 62192783 and 62506157, the Jiangsu Science and Technology Major Project BG2024031, the Fundamental Research Funds for the Cen-tral Universities (14380128), the Collaborative Innovation Center of Novel Software Technology and Industrialization, and the InnoHK funding.

References

Bowling, M.; Burch, N.; Johanson, M.; and Tammelin, O. 2015. Heads-Up limit Holdâ€™em Poker is Solved. Science ,347(6218): 145â€“149. Brown, N.; and Sandholm, T. 2018. Superhuman AI for Heads-Up No-Limit Poker: Libratus Beats Top Professionals.

Science , 359(6374): 418â€“424. Brown, N.; and Sandholm, T. 2019a. Solving Imperfect-Information Games via Discounted Regret Minimization. In

Proceedings of the 33rd AAAI Conference on Artificial Intel-ligence , 1829â€“1836. Brown, N.; and Sandholm, T. 2019b. Superhuman AI for Multiplayer Poker. Science , 365(6456): 885â€“890. Chen, X.; Han, Z.; Zhang, H.; Xue, G.; Xiao, Y.; and Ben-nis, M. 2017. Wireless Resource Scheduling in Virtualized Radio Access Networks Using Stochastic Learning. IEEE Transactions on Mobile Computing , 17(4): 961â€“974. Farina, G.; Grand-Cl Â´ement, J.; Kroer, C.; Lee, C.-W.; and Luo, H. 2023. Regret Matching+: (In)Stability and Fast Con-vergence in Games. In Proceedings of the 37th Conference on Neural Information Processing Systems .Farina, G.; Kroer, C.; and Sandholm, T. 2019. Online Con-vex Optimization for Sequential Decision Processes and Extensive-Form Games. In Proceedings of the 33rd AAAI Conference on Artificial Intelligence , 1917â€“1925. Farina, G.; Kroer, C.; and Sandholm, T. 2021. Faster Game Solving via Predictive Blackwell Approachability: Connect-ing Regret Matching and Mirror Descent. In Proceedings of the 35th AAAI Conference on Artificial Intelligence , 5363â€“ 5371. Gordon, G. J. 2006. No-regret Algorithms for Online Convex Programs. In Proceedings of the 19th International Confer-ence on Neural Information Processing Systems , 489â€“496. MIT Press. Hart, S.; and Mas-Colell, A. 2000. A simple adaptive proce-dure leading to correlated equilibrium. Econometrica , 68(5): 1127â€“1150. Hazan, E.; et al. 2016. Introduction to online convex opti-mization. Foundations and TrendsÂ® in Optimization , 2(3-4): 157â€“325. Joulani, P.; Gy Â¨orgy, A.; and Szepesv Â´ari, C. 2017. A Modular Analysis of Adaptive (Non-)Convex Optimization: Optimism, Composite Objectives, Variance Reduction, and Variational Bounds. In Proceedings of the 30th International Conference on Algorithmic Learning Theory , 681â€“720. Lanctot, M.; Lockhart, E.; Lespiau, J.-B.; Zambaldi, V.; Upad-hyay, S.; P Â´erolat, J.; Srinivasan, S.; Timbers, F.; Tuyls, K.; Omidshafiei, S.; et al. 2019. OpenSpiel: A Framework for Reinforcement Learning in Games. Lanctot, M.; Waugh, K.; Zinkevich, M.; and Bowling, M. 2009. Monte Carlo Sampling for Regret Minimization in Extensive Games. In Proceedings of the 22nd International Conference on Neural Information Processing Systems , 1078â€“ 1086. Lis `y, V.; Davis, T.; and Bowling, M. 2016. Counterfactual Re-gret Minimization in Sequential Security Games. In Proceed-ings of the 30th AAAI Conference on Artificial Intelligence ,544â€“550. Liu, M.; Farina, G.; and Ozdaglar, A. 2024. LiteEFG: An Efficient Python Library for Solving Extensive-form Games.

arXiv preprint arXiv:2407.20351 .Liu, M.; Ozdaglar, A. E.; Yu, T.; and Zhang, K. 2023. The Power of Regularization in Solving Extensive-Form Games. In Proceedings of the 12th International Conference on Learning Representations .Liu, W.; Jiang, H.; Li, B.; and Li, H. 2021. Equivalence Analysis between Counterfactual Regret Minimization and Online Mirror Descent. arXiv preprint arXiv:2110.04961 .Meng, L.; Zhang, Y.; Ge, Z.; Yang, S.; Ding, T.; Li, W.; Yang, T.; An, B.; and Gao, Y. 2023. Efficient Last-iterate Conver-gence Algorithms in Solving Games. arXiv:2308.11256. Morav Ë‡cÂ´Ä±k, M.; Schmid, M.; Burch, N.; Lis `y, V.; Morrill, D.; Bard, N.; Davis, T.; Waugh, K.; Johanson, M.; and Bowling, M. 2017. Deepstack: Expert-level artificial intelligence in heads-up no-limit poker. Science , 356(6337): 508â€“513. Nemirovskij, A. S.; and Yudin, D. B. 1983. Problem com-plexity and method efficiency in optimization. Osborne, M. J.; et al. 2004. An introduction to game theory ,volume 3. Oxford university press New York. P Â´erolat, J.; De Vylder, B.; Hennes, D.; Tarassov, E.; Strub, F.; de Boer, V.; Muller, P.; Connor, J. T.; Burch, N.; Anthony, T.; et al. 2022. Mastering the game of Stratego with model-free multiagent reinforcement learning. Science , 378(6623): 990â€“996. P Â´erolat, J.; Munos, R.; Lespiau, J.; Omidshafiei, S.; Rowland, M.; Ortega, P. A.; Burch, N.; Anthony, T. W.; Balduzzi, D.; Vylder, B. D.; Piliouras, G.; Lanctot, M.; and Tuyls, K. 2021. From Poincar Â´e Recurrence to Convergence in Imperfect In-formation Games: Finding Equilibrium via Regularization. In Proceedings of the 38th International Conference on Ma-chine Learning , 8525â€“8535. Rakhlin, A.; and Sridharan, K. 2013a. Online learning with predictable sequences. In Proceedings of the 26th Annual Conference on Learning Theory , 993â€“1019. Rakhlin, A.; and Sridharan, K. 2013b. Optimization, learning, and games with predictable sequences. In Proceedings of the 26th International Conference on Neural Information Processing Systems , 3066â€“3074. Sandholm, T. 2015. Steering Evolution Strategically: Compu-tational Game Theory and Opponent Exploitation for Treat-ment Planning, Drug Design, and Synthetic Biology. In Proceedings of the 29th AAAI Conference on Artificial Intel-ligence , 4057â€“4061. Steinberger, E. 2019. PokerRL. https://github.com/ TinkeringCode/PokerRL. Tammelin, O. 2014. Solving large imperfect information games using CFR+. arXiv preprint arXiv:1407.5042 .Tammelin, O.; Burch, N.; Johanson, M.; and Bowling, M. 2015. Solving heads-up limit Texas Holdâ€™em. In Proceedings of the 24th International Conference on Artificial Intelligence ,645â€“652. Wei, C.; Lee, C.; Zhang, M.; and Luo, H. 2021. Linear Last-iterate Convergence in Constrained Saddle-point Opti-mization. In Proceedings of the 9th International Conference on Learning Representations .Xu, H.; Li, K.; Fu, H.; Fu, Q.; and Xing, J. 2022. AutoCFR: learning to design counterfactual regret minimization algo-rithms. In Proceedings of the 36th AAAI Conference on Artificial Intelligence , volume 36, 5244â€“5251. Xu, H.; Li, K.; Fu, H.; Fu, Q.; Xing, J.; and Cheng, J. 2024a. Dynamic discounted counterfactual regret minimization. In

Proceedings of the 12th International Conference on Learn-ing Representations .Xu, H.; Li, K.; Liu, B.; Fu, H.; Fu, Q.; Xing, J.; and Cheng, J. 2024b. Minimizing weighted counterfactual regret with opti-mistic online mirror descent. In Proceedings of the 33rd In-ternational Joint Conference on Artificial Intelligence , 5272â€“ 5280. Zhang, N.; McAleer, S.; and Sandholm, T. 2024. Faster Game Solving via Hyperparameter Schedules. arXiv preprint arXiv:2404.09097 .Zinkevich, M.; Johanson, M.; Bowling, M.; and Piccione, C. 2007. Regret Minimization in Games with Incomplete Infor-mation. In Proceedings of the 20th International Conference on Neural Information Processing Systems , 1729â€“1736. A Proof of Theorem 4.1

Proof. To prove Theorem 4.1, we use the equivalence between RM + and Online Mirror Descent (OMD) proposed by Farina, Kroer, and Sandholm (2021). We first introduce OMD. OMD is a traditional regret minimization algorithm (Nemirovskij and Yudin 1983). Let â„“t âˆˆ Rd, xt âˆˆ D , and let Ïˆ : D â†’ Rd

> â‰¥0

= {y|y âˆˆ Rd, y â‰¥ 0} be a 1-strongly convex differentiable regularizer with respect to some norm âˆ¥ Â· âˆ¥ , OMD generates the decisions via

xt+1 := arg min

> xâ€²âˆˆD



âŸ¨â„“t, xâ€²âŸ© + 1

Î· BÏˆ (xâ€² âˆ¥ xt)



,

where BÏˆ (u, v) = Ïˆ(u) âˆ’ Ïˆ(v) âˆ’ âŸ¨âˆ‡ Ïˆ(v), u âˆ’ vâŸ© is the Bregman divergence associated with Ïˆ(Â·).From the analysis in Section D of Farina, Kroer, and Sandholm (2021), by setting Ïˆ(Â·) as the quadratic regularizer 12 âˆ¥ Â· âˆ¥ 22, the update rule APCFR + at infoset I can be written as

Ë†Î¸tI âˆˆ arg min

> Î¸IâˆˆR|A(I)|â‰¥0

{âŸ¨âˆ’ 11 + Î±tI

rtâˆ’1

> I

, Î¸I âŸ© + 1

Î· BÏˆ (Î¸I , Î¸tI )},

Î¸t+1

> I

âˆˆ arg min

> Î¸IâˆˆR|A(I)|â‰¥0

{âŸ¨âˆ’ rtI , Î¸I âŸ© + 1

Î· BÏˆ (Î¸I , Î¸tI )},

(4) where Î· > 0 is a constant and Ïƒti (I) = Ë†Î¸tI

> âˆ¥Ë†Î¸tIâˆ¥1

. Note that if Î¸0

> I

= 0 for all I âˆˆ I , then for any Î·, the strategy profile sequence

{Ïƒ1, Ïƒ 2, Â· Â· Â· , Ïƒ T } generated by APCFR + is same and Î·RtI = Î¸tI (Farina, Kroer, and Sandholm 2021).

Lemma A.1. [Lemma 4 of Farina, Kroer, and Sandholm (2021)] Let D âŠ† Rd be closed and convex, let â„“t âˆˆ Rd, xt âˆˆ D , and let Ïˆ : D â†’ Râ‰¥0 be a 1-strongly convex differentiable regularizer with respect to some norm âˆ¥ Â· âˆ¥ . Then,

xt+1 âˆˆ arg min

> xâˆˆD



âŸ¨â„“t, xâŸ© + 1

Î· BÏˆ (x âˆ¥ xt)



is well defined (that is, the minimizer exists and is unique), and for all xâ€² âˆˆ D satisfies the inequality

âŸ¨â„“t,xt+1 âˆ’xâ€²âŸ©â‰¤ 1

Î·

ï¿½BÏˆ (xâ€²,xt)âˆ’B Ïˆ (xâ€²,xt+1 )âˆ’B Ïˆ (xt+1 ,xt).

Considering the second line of Eq. (4), and using Lemma A.1 with xt = Î¸tI , xt+1 = Î¸t+1

> I

, xâ€² = Ïƒi(I) and â„“t = âˆ’rtI , we have

âŸ¨âˆ’ rtI ,Î¸t+1

> I

âˆ’Ïƒi(I)âŸ©â‰¤ 1

Î·

ï¿½BÏˆ (Ïƒi(I),Î¸tI )âˆ’B Ïˆ (Ïƒi(I),Î¸t+1

> I

)âˆ’B Ïˆ (Î¸t+1

> I

,Î¸tI )

â‡”âŸ¨âˆ’ rtI ,Î¸t+1

> I

âˆ’ Ë†Î¸tI + Ë†Î¸tI âˆ’Ïƒi(I)âŸ©â‰¤ 1

Î·

ï¿½BÏˆ (Ïƒi(I),Î¸tI )âˆ’B Ïˆ (Ïƒi(I),Î¸t+1

> I

)âˆ’B Ïˆ (Î¸t+1

> I

,Î¸tI ).

(5) Similarly, considering the first line of Eq. (4), and using Lemma A.1 with xt = Î¸tI , xt+1 = Ë†Î¸tI , xâ€² = Î¸t+1

> I

and â„“t = âˆ’rtâˆ’1

> I

, we get

11 + Î±tI

âŸ¨âˆ’ rtâˆ’1

> I

, Ë†Î¸tI âˆ’ Î¸t+1

> I

âŸ© â‰¤ 1

Î·



BÏˆ (Î¸t+1

> I

, Î¸tI ) âˆ’ B Ïˆ (Î¸t+1

> I

, Ë†Î¸tI ) âˆ’ B Ïˆ ( Ë†Î¸tI , Î¸tI )



â‡”âŸ¨âˆ’ rtâˆ’1

> I

, Ë†Î¸tI âˆ’ Î¸t+1

> I

âŸ© â‰¤ 1 + Î±tI

Î·



BÏˆ (Î¸t+1

> I

, Î¸tI ) âˆ’ B Ïˆ (Î¸t+1

> I

, Ë†Î¸tI ) âˆ’ B Ïˆ ( Ë†Î¸tI , Î¸tI )



.

(6) Summing up Eq. (5) with Eq. (6), we have

âŸ¨âˆ’ rtI , Î¸t+1

> I

âˆ’ Ë†Î¸tI + Ë†Î¸tI âˆ’ Ïƒi(I)âŸ© + âŸ¨âˆ’ rtâˆ’1

> I

, Ë†Î¸tI âˆ’ Î¸t+1

> I

âŸ©â‰¤ 1

Î·

ï¿½BÏˆ (Ïƒi(I), Î¸tI ) âˆ’ B Ïˆ (Ïƒi(I), Î¸t+1

> I

) âˆ’ B Ïˆ (Î¸t+1

> I

, Î¸tI ) +1 + Î±tI

Î·



BÏˆ (Î¸t+1

> I

, Î¸tI ) âˆ’ B Ïˆ (Î¸t+1

> I

, Ë†Î¸tI ) âˆ’ B Ïˆ ( Ë†Î¸tI , Î¸tI )



,which implies

âŸ¨âˆ’ rtI , Ë†Î¸tI âˆ’Ïƒi(I)âŸ©â‰¤âŸ¨ rtI ,Î¸t+1

> I

âˆ’ Ë†Î¸tI âŸ©+âŸ¨rtâˆ’1

> I

, Ë†Î¸tI âˆ’Î¸t+1

> I

âŸ©+ 1

Î·

ï¿½BÏˆ (Ïƒi(I),Î¸tI )âˆ’B Ïˆ (Ïƒi(I),Î¸t+1

> I

)âˆ’B Ïˆ (Î¸t+1

> I

,Î¸tI )+1+ Î±tI

Î·



BÏˆ (Î¸t+1

> I

,Î¸tI )âˆ’B Ïˆ (Î¸t+1

> I

, Ë†Î¸tI )âˆ’B Ïˆ ( Ë†Î¸tI ,Î¸tI )



â‰¤âŸ¨ rtI âˆ’rtâˆ’1

> I

,Î¸t+1

> I

âˆ’ Ë†Î¸tI âŸ©+ 1

Î·

ï¿½BÏˆ (Ïƒi(I),Î¸tI )âˆ’B Ïˆ (Ïƒi(I),Î¸t+1

> I

âˆ’B Ïˆ (Î¸t+1

> I

,Î¸tI )+1+ Î±tI

Î·



BÏˆ (Î¸t+1

> I

,Î¸tI )âˆ’B Ïˆ (Î¸t+1

> I

, Ë†Î¸tI )âˆ’B Ïˆ ( Ë†Î¸tI ,Î¸tI )



.

(7) From the facts that (i) rtI = vÏƒt

(I) âˆ’ âŸ¨ vÏƒt

(I), Ïƒ ti (I)âŸ©1 and (ii) Ïƒti (I) = [ Ë†Î¸tI ]+

> âˆ¥[ Ë†Î¸tI]+âˆ¥1

= Ë†Î¸tI

> âˆ¥Ë†Î¸tIâˆ¥1

, we have

âŸ¨âˆ’ rtI , Ë†Î¸tI âˆ’ Ïƒi(I)âŸ© = âˆ’âŸ¨âŸ¨ vÏƒt

(I), Ïƒ ti (I)âŸ©1 âˆ’ vÏƒt

(I), Ïƒ i(I) âˆ’ Ë†Î¸tI âŸ©

= âˆ’âŸ¨âŸ¨ vÏƒt

(I), Ë†Î¸tI

âˆ¥ Ë†Î¸tI âˆ¥1

âŸ©1 âˆ’ vÏƒt

(I), Ïƒ i(I) âˆ’ Ë†Î¸tI âŸ©

= âˆ’âŸ¨âŸ¨ vÏƒt

(I), Ë†Î¸tI

âˆ¥ Ë†Î¸tI âˆ¥1

âŸ©1 âˆ’ vÏƒt

(I), Ïƒ i(I)âŸ© âˆ’ âŸ¨âŸ¨ vÏƒt

(I), Ë†Î¸tI

âˆ¥ Ë†Î¸tI âˆ¥1

âŸ©1 âˆ’ vÏƒt

(I), Ë†Î¸tI âŸ©

= âˆ’âŸ¨âŸ¨ vÏƒt

(I), Ë†Î¸tI

âˆ¥ Ë†Î¸tI âˆ¥1

âŸ©1 âˆ’ vÏƒt

(I), Ïƒ i(I)âŸ© âˆ’ âŸ¨âŸ¨ vÏƒt

(I), Ë†Î¸tI

âˆ¥ Ë†Î¸tI âˆ¥1

âŸ©1, Ë†Î¸tI âŸ© + âŸ¨vÏƒt

(I), Ë†Î¸tI âŸ©

= âˆ’âŸ¨âŸ¨ vÏƒt

(I), Ë†Î¸tI

âˆ¥ Ë†Î¸tI âˆ¥1

âŸ©1 âˆ’ vÏƒt

(I), Ïƒ i(I)âŸ© âˆ’ âŸ¨ vÏƒt

(I), Ë†Î¸tI

âˆ¥ Ë†Î¸tI âˆ¥1

âŸ©âˆ¥ Ë†Î¸tI âˆ¥1 + âŸ¨vÏƒt

(I), Ë†Î¸tI âŸ©

= âˆ’âŸ¨âŸ¨ vÏƒt

(I), Ë†Î¸tI

âˆ¥ Ë†Î¸tI âˆ¥1

âŸ©1 âˆ’ vÏƒt

(I), Ïƒ i(I)âŸ© âˆ’ âŸ¨ vÏƒt

(I), Ë†Î¸tI âŸ© + âŸ¨vÏƒt

(I), Ë†Î¸tI âŸ©

= âˆ’âŸ¨âŸ¨ vÏƒt

(I), Ë†Î¸tI

âˆ¥ Ë†Î¸tI âˆ¥1

âŸ©1 âˆ’ vÏƒt

(I), Ïƒ i(I)âŸ©

= âˆ’âŸ¨ vÏƒt

(I), Ïƒ ti (I) âˆ’ Ïƒi(I)âŸ© = âŸ¨vÏƒt

(I), Ïƒ i(I) âˆ’ Ïƒti (I)âŸ©,

where the fifth line comes from âŸ¨1, Ë†Î¸tI âŸ© = âˆ¥ Ë†Î¸tI âˆ¥1, as well as the last line is from Ë†Î¸tI

> âˆ¥Ë†Î¸tIâˆ¥1

= Ïƒti (I) and âŸ¨1, Ïƒ i(I)âŸ© = 1 (as

Ïƒi(I) âˆˆ âˆ†|A(I)|). In addition, we have

RT (I) = max

> aâˆˆA(I)
> T

X

> t=1

vÏƒt

(Ia ) âˆ’

> T

X

> t=1

X

> aâˆˆA(I)

Ïƒti (Ia )vÏƒt

(Ia )

â‰¤ max

> Ïƒi(I)
> T

X

> t=1

âŸ¨vÏƒt

(I), Ïƒ i(I) âˆ’ Ïƒti (I)âŸ©.

(8) Therefore, from Eq. (7) and âŸ¨âˆ’ rtI , Ë†Î¸tI âˆ’ Ïƒi(I)âŸ© = âŸ¨vÏƒt

(I), Ïƒ i(I) âˆ’ Ïƒti (I)âŸ©, we can bound

âŸ¨rtI âˆ’rtâˆ’1

> I

,Î¸t+1

> I

âˆ’ Ë†Î¸tI âŸ©+ 1

Î·

ï¿½BÏˆ (Ïƒi(I),Î¸tI )âˆ’B Ïˆ (Ïƒi(I),Î¸t+1

> I

)âˆ’B Ïˆ (Î¸t+1

> I

,Î¸tI )+1+ Î±tI

Î·



BÏˆ (Î¸t+1

> I

,Î¸tI )âˆ’B Ïˆ (Î¸t+1

> I

, Ë†Î¸tI )âˆ’B Ïˆ ( Ë†Î¸tI ,Î¸tI )



,

(9) to bound RT (I).For Eq. (9), summing up from t = 1 to t = T , we get

> T

X

> t=1

âŸ¨rtI âˆ’rtâˆ’1

> I

,Î¸t+1

> I

âˆ’ Ë†Î¸tI âŸ©+

> T

X

> t=1

1

Î·

ï¿½BÏˆ (Ïƒi(I),Î¸tI )âˆ’B Ïˆ (Ïƒi(I),Î¸t+1

> I

)âˆ’B Ïˆ (Î¸t+1

> I

,Î¸tI )+

> T

X

> t=1

1+ Î±tI

Î·



BÏˆ (Î¸t+1

> I

,Î¸tI )âˆ’B Ïˆ (Î¸t+1

> I

, Ë†Î¸tI )âˆ’B Ïˆ ( Ë†Î¸tI ,Î¸tI )



â‰¤ 1

Î· BÏˆ (Ïƒi(I),Î¸1

> I

)+

> T

X

> t=1

âˆ¥rtI âˆ’rtâˆ’1

> I

âˆ¥2âˆ¥Î¸t+1

> I

âˆ’ Ë†Î¸tI âˆ¥2+

> T

X

> t=1

Î±tI

Î· BÏˆ (Î¸t+1

> I

,Î¸tI )+

> T

X

> t=1

1+ Î±tI

Î·



âˆ’B Ïˆ (Î¸t+1

> I

, Ë†Î¸tI )âˆ’B Ïˆ ( Ë†Î¸tI ,Î¸tI )



â‰¤ 1

Î· BÏˆ (Ïƒi(I),Î¸1

> I

)+

> T

X

> t=1

Î· âˆ¥rtI âˆ’rtâˆ’1

> I

âˆ¥22

2(1+ Î±tI ) +

> T

X

> t=1

(1+ Î±tI ) âˆ¥Î¸t+1

> I

âˆ’ Ë†Î¸tI âˆ¥2

2Î· +

> T

X

> t=1

Î±tI

Î· BÏˆ (Î¸t+1

> I

,Î¸tI )+

> T

X

> t=1

1+ Î±tI

Î·



âˆ’B Ïˆ (Î¸t+1

> I

, Ë†Î¸tI )âˆ’B Ïˆ ( Ë†Î¸tI ,Î¸tI )



â‰¤ 1

Î· BÏˆ (Ïƒi(I),Î¸1

> I

)+

> T

X

> t=1

Î· âˆ¥rtI âˆ’rtâˆ’1

> I

âˆ¥22

2(1+ Î±tI ) +

> T

X

> t=1

Î±tI

Î· BÏˆ (Î¸t+1

> I

,Î¸tI )

â‰¤ 1

Î· BÏˆ (Ïƒi(I),Î¸1

> I

)+

> T

X

> t=1

Î· âˆ¥rtI âˆ’rtâˆ’1

> I

âˆ¥22

2(1+ Î±tI ) +

> T

X

> t=1

Î· Î±tI âˆ¥Rt+1

> I

âˆ’RtI âˆ¥22

2 ,

(10) where the second inequality comes from that âˆ€b, c, Ï > 0, bc â‰¤ b2/(2 Ï) + Ïc 2/2 (b = âˆ¥rtI âˆ’ rtâˆ’1

> I

âˆ¥2, c = âˆ¥Î¸t+1

> I

âˆ’ Ë†Î¸tI âˆ¥2, and

Ï = (1 + Î±tI )/Î· ), the third inequality is from BÏˆ (Î¸t+1

> I

, Ë†Î¸tI ) = âˆ¥Î¸t+1

> I

âˆ’ Ë†Î¸tI âˆ¥22/2 (which is from the fact that in PCFR + variants,

Ïˆ(Â·) = âˆ¥ Â· âˆ¥ 22/2), as well as the last line comes from the facts that BÏˆ (Î¸t+1

> I

, Î¸tI ) = âˆ¥Î¸t+1

> I

âˆ’ Î¸tI âˆ¥22/2 and Î·RtI = Î¸tI (see more details in Section D of Farina, Kroer, and Sandholm (2021)). For the term BÏˆ (Ïƒi(I), Î¸1

> I

) and , we have

BÏˆ (Ïƒi(I), Î¸1

> I

) = âˆ¥Ïƒi(I) âˆ’ Î¸1

> I

âˆ¥22

2 = âˆ¥Ïƒi(I)âˆ¥22

2 â‰¤ 12 , (11) where the second equality comes from the definition of Î¸1

> I

, i.e. , Î¸1

> I

= 0, and the inequality is from âˆ¥Ïƒi(I)âˆ¥1 â‰¤ 1, âˆ€Ïƒi(I) âˆˆ

âˆ†|A(I)|. By substituting Eq. (11) into Eq. (10), we have

RT (I) â‰¤ 12Î· +

> T

X

> t=1

Î· âˆ¥rtI âˆ’ rtâˆ’1

> I

âˆ¥22

2(1 + Î±tI ) +

> T

X

> t=1

Î· Î±tI âˆ¥Rt+1

> I

âˆ’ RtI âˆ¥22

2 .

Obviously, by setting

1

Î· =

vuut TX

> t=1

 âˆ¥rtI âˆ’ rtâˆ’1

> I

âˆ¥22

1 + Î±tI

+ Î±tI âˆ¥Rt+1

> I

âˆ’ RtI âˆ¥22



,

we have that the lower bound of the upper bound of RT (I) is

vuut TX

> t=1

 âˆ¥rtI âˆ’ rtâˆ’1

> I

âˆ¥22

1 + Î±tI

+ Î±tI âˆ¥Rt+1

> I

âˆ’ RtI âˆ¥22



.

It completes the proof.

B An Alternative Regret Upper Bound of APCFR +

Theorem B.1. Assume that T iterations of APCFR + with any Î±tI â‰¥ 0 are conducted. Then the counterfactual regret at any infoset I âˆˆ I is bound by

RT (I) â‰¤

vuut TX

> t=1

âˆ¥rtI âˆ’ 11 + Î±tI

rtâˆ’1

> I

âˆ¥22.Obviously, by setting Î± â†’ âˆ , we derive the original bound of CFR + in its original version. Moreover, the bound in Theorem B.1 can be combined with the bound in Theorem 4.1 to provide a new regret upper bound for APCFR +. Specifically, the regret of APCFR + is smaller than the minimum of the bounds presented in Theorem 4.1 and Theorem B.1:

RT (I) â‰¤ min

ï£«ï£­vuut TX

> t=1

 âˆ¥rtI âˆ’ rtâˆ’1

> I

âˆ¥22

1 + Î±tI

+ Î±tI âˆ¥Rt+1

> I

âˆ’ RtI âˆ¥22



,

vuut TX

> t=1

âˆ¥rtI âˆ’ 11 + Î±tI

rtâˆ’1

> I

âˆ¥22

ï£¶ï£¸ .

We employ Theorem 4.1 rather than Theorem B.1 in the main text because the regret upper bound presented in The-orem B.1 is typically significantly larger than that in Theorem 4.1. In our experiments (as shown in Figures 8, 9, 10, and 11), we observe that the value of PTt=1 âˆ¥rtI âˆ’ 11+ Î±tI

rtâˆ’1

> I

âˆ¥22 consistently increases over time. In contrast, the value of

PTt=1

 âˆ¥rtI âˆ’rtâˆ’1

> Iâˆ¥22
> 1+ Î±tI

+ Î±tI âˆ¥Rt+1

> I

âˆ’ RtI âˆ¥22



tends to stabilize, exhibiting a flattening trend. We hypothesize that this phenomenon arises from the fact that even when rtI and rtâˆ’1

> I

are close, the value of âˆ¥rtI âˆ’ 11+ Î±tI

rtâˆ’1

> I

âˆ¥22 remains extremely large.

Proof. Firstly, from Eq. (6), we get

11 + Î±tI

âŸ¨âˆ’ rtâˆ’1

> I

, Ë†Î¸tI âˆ’ Î¸t+1

> I

âŸ© â‰¤ 1

Î·



BÏˆ (Î¸t+1

> I

, Î¸tI ) âˆ’ B Ïˆ (Î¸t+1

> I

, Ë†Î¸tI ) âˆ’ B Ïˆ ( Ë†Î¸tI , Î¸tI )



. (12) By summing up Eq. (5) with Eq. (12), we have

âŸ¨âˆ’ rtI , Î¸t+1

> I

âˆ’ Ë†Î¸tI + Ë†Î¸tI âˆ’ Ïƒi(I)âŸ© + 11 + Î±tI

âŸ¨âˆ’ rtâˆ’1

> I

, Ë†Î¸tI âˆ’ Î¸t+1

> I

âŸ©â‰¤ 1

Î·

ï¿½BÏˆ (Ïƒi(I), Î¸tI ) âˆ’ B Ïˆ (Ïƒi(I), Î¸t+1

> I

) âˆ’ B Ïˆ (Î¸t+1

> I

, Î¸tI ) +1

Î·



BÏˆ (Î¸t+1

> I

, Î¸tI ) âˆ’ B Ïˆ (Î¸t+1

> I

, Ë†Î¸tI ) âˆ’ B Ïˆ ( Ë†Î¸tI , Î¸tI )



â‰¤ 1

Î·

ï¿½BÏˆ (Ïƒi(I), Î¸tI ) âˆ’ B Ïˆ (Ïƒi(I), Î¸t+1

> I

) + 1

Î·



âˆ’B Ïˆ (Î¸t+1

> I

, Ë†Î¸tI ) âˆ’ B Ïˆ ( Ë†Î¸tI , Î¸tI )



.

Then, we have

âŸ¨âˆ’ rtI , Ë†Î¸tI âˆ’ Ïƒi(I)âŸ©â‰¤âŸ¨ rtI , Î¸t+1

> I

âˆ’ Ë†Î¸tI âŸ© + 11 + Î±tI

âŸ¨rtâˆ’1

> I

, Ë†Î¸tI âˆ’ Î¸t+1

> I

âŸ© + 1

Î·

ï¿½BÏˆ (Ïƒi(I), Î¸tI ) âˆ’ B Ïˆ (Ïƒi(I), Î¸t+1

> I

) +1

Î·



âˆ’B Ïˆ (Î¸t+1

> I

, Ë†Î¸tI ) âˆ’ B Ïˆ ( Ë†Î¸tI , Î¸tI )



.

(13) Combining Eq. (7), (8) and (13), we have

RT (I) â‰¤ 1

Î· BÏˆ (Ïƒi(I), Î¸1

> I

) +

> T

X

> t=1

âŸ¨rtI , Î¸t+1

> I

âˆ’ Ë†Î¸tI âŸ©+

> T

X

> t=1

11 + Î±tI

âŸ¨rtâˆ’1

> I

, Ë†Î¸tI âˆ’ Î¸t+1

> I

âŸ© +

> T

X

> t=1

1

Î·



âˆ’B Ïˆ (Î¸t+1

> I

, Ë†Î¸tI ) âˆ’ B Ïˆ ( Ë†Î¸tI , Î¸tI )



â‰¤ 1

Î· BÏˆ (Ïƒi(I), Î¸1

> I

) +

> T

X

> t=1

âŸ¨rtI âˆ’ 11 + Î±tI

rtâˆ’1

> I

, Î¸t+1

> I

âˆ’ Ë†Î¸tI âŸ© +

> T

X

> t=1

1

Î·



âˆ’B Ïˆ (Î¸t+1

> I

, Ë†Î¸tI ) âˆ’ B Ïˆ ( Ë†Î¸tI , Î¸tI )



â‰¤ 1

Î· BÏˆ (Ïƒi(I), Î¸1

> I

) +

> T

X

> t=1

âˆ¥rtI âˆ’ 11 + Î±tI

rtâˆ’1

> I

âˆ¥2âˆ¥Î¸t+1

> I

âˆ’ Ë†Î¸tI âˆ¥2 +

> T

X

> t=1

1

Î·



âˆ’B Ïˆ (Î¸t+1

> I

, Ë†Î¸tI ) âˆ’ B Ïˆ ( Ë†Î¸tI , Î¸tI )



â‰¤ 1

Î· BÏˆ (Ïƒi(I), Î¸1

> I

) +

> T

X

> t=1

Î· âˆ¥rtI âˆ’ 11+ Î±tI

rtâˆ’1

> I

âˆ¥22

2 +

> T

X

> t=1

âˆ¥Î¸t+1

> I

âˆ’ Ë†Î¸tI âˆ¥2

2Î· +

> T

X

> t=1

1

Î·



âˆ’B Ïˆ (Î¸t+1

> I

, Ë†Î¸tI ) âˆ’ B Ïˆ ( Ë†Î¸tI , Î¸tI )



â‰¤ 1

Î· BÏˆ (Ïƒi(I), Î¸1

> I

) +

> T

X

> t=1

Î· âˆ¥rtI âˆ’ 11+ Î±tI

rtâˆ’1

> I

âˆ¥22

2 ,where the third inequality comes from that âˆ€b, c, Ï > 0, bc â‰¤ b2/(2 Ï) + Ïc 2/2 (in this case, b = âˆ¥rtI âˆ’ 11+ Î±tI

rtâˆ’1

> I

âˆ¥2,

c = âˆ¥Î¸t+1

> I

âˆ’ Ë†Î¸tI âˆ¥2, and Ï = 1 /Î· ), and the last inequality is from BÏˆ (Î¸t+1

> I

, Ë†Î¸tI ) = âˆ¥Î¸t+1

> I

âˆ’ Ë†Î¸tI âˆ¥22/2. Then, from Eq. (11) (BÏˆ (Ïƒi(I), Î¸1

> I

) â‰¤ 12 ), we get

RT (I) â‰¤ 12Î· +

> T

X

> t=1

Î· âˆ¥rtI âˆ’ 11+ Î±tI

rtâˆ’1

> I

âˆ¥22

2 .

Obviously, by setting

1

Î· =

vuut TX

> t=1

âˆ¥rtI âˆ’ 11 + Î±tI

rtâˆ’1

> I

âˆ¥22,

we have that the lower bound of the upper bound of RT (I) is

vuut TX

> t=1

âˆ¥rtI âˆ’ 11 + Î±tI

rtâˆ’1

> I

âˆ¥22.

It completes the proof.

C Details of APDCFR +

Next, we introduce APDCFR +. At each iteration t, the strategy is updated at each infoset I âˆˆ I as follows:

Ë†RtI =

 Î»t Î²

Îº + tÎ² RtI + 11 + Î±tI

rtâˆ’1

> I

+

, Rt+1

> I

=



RtI + Î»t Î²

Îº + tÎ² rtI

+

,Ïƒti (I) =

h Ë†RtI

i+

âˆ¥

h Ë†RtI

i+

âˆ¥1

= Ë†RtI

âˆ¥ Ë†RtI âˆ¥1

, Î± tI = min

s Ptâˆ’1

> Ï„=1

âˆ¥rÏ„I âˆ’ rÏ„ âˆ’1

> I

âˆ¥22

Ptâˆ’1

> Ï„=1

âˆ¥RÏ„ +1

> I

âˆ’ RÏ„I âˆ¥22

, 9

!

,

where i = P (I), R1

> I

= 0, and r0

> I

= 0. In updating Î±tI , we opt for Î±max = 9 instead of 5 as our observations suggest that 9

yields superior performance within HUNL Subgames. Furthermore, we utilize the expression Î»t Î²

> Îº+tÎ²

for discounting accumulated regrets, in contrast to the tÎ²

> 1+ tÎ²

used in DCFR. By setting different values for Î» and Îº, we achieve a more aggressive discounting strategy than that in the original DCFR. Specifically, within this study, we assign Î» = 20 and Îº = 500 . This configuration implies that for Rt+1

> I

, the impact of rtI near the initial stagesâ€”where t approaches 1â€”is relatively extremely smaller compared to employing tÎ²

> 1+ tÎ²

. Formally, by setting Î» = 20 and Îº = 500 , the range of the values of Î»t Î²

> Îº+tÎ²

is [ 20 501 , 20] , while the range of the values of tÎ²

> 1+ tÎ²

is [ 12 , 1] . APDCFR + uses the following weighted averaging strategy:

Â¯Ïƒt =

 t âˆ’ 1

t

2.5

Â¯Ïƒtâˆ’1 + Ïƒt.

Notably, the parameters Î±max = 9 , Î» = 20 , Îº = 500 , and Î² = 1 .5 are selected from the ranges [5 , 7, 9] , [10 , 20 , 50 , 100] ,

[100 , 500 , 1000] , and [1 .0, 1.5, 2.0, 2.5] , respectively. The parameter tuning of APDCFR + is conducted exclusively on HUNL subgames to identify the optimal configuration.

D Additional Experiments

Sizes of the Games. Before introducing our additional experiments, we present the sizes of the games used in our experiments, as shown in Table 3. In this table, #Histories denotes the number of histories in the game tree, while #Infosets represents the number of infosets in the game tree. #Terminal histories indicates the number of terminal histories in the game tree. #Depth defines the depth of the game tree, which is the maximum number of actions in a single history. Lastly, #Max size of infosets indicates the maximum number of histories that belong to the same infoset.

Running times. We now compare the running time of our algorithms with that of PCFR +, both executed with the same number of iterations (i.e., 5000). The experimental results are shown in Table 4. We observe that the running time of APCFR + is slightly higher compared to PCFR +, primarily due to the additional Î±tI learning process in APCFR +, which increases its running time. However, we also note that the running time of SAPCFR + is nearly identical to that of PCFR +, as the only difference between their implementations is a single line of code, which does not alter the computational complexity. Importantly, the computational complexity remains exactly the same, even with no change in the constant factors. Game #Histories #Infosets #Terminal histories #Depth #Max size of infosets

Kuhn Poker 58 12 30 6 2Leduc Poker 9,457 936 5,520 12 5Battleship (3,2,3) 732,607 81,027 552,132 9 7Battleship (4,3,2) 5,462,407 58,159 4,966,176 7 17 Liarâ€™s Dice (4) 8,181 1,024 4,080 12 4Liarâ€™s Dice (5) 51,181 5,120 25,575 14 5Goofspiel (4) 1,077 162 576 7 14 Goofspiel (5) 26,931 2,124 14,400 9 46 Subgame 3 398,112,843 69,184 261,126,360 10 1,980 Subgame 4 244,005,483 43,240 158,388,120 8 1,980 Leduc Poker (5) 55,361 2,760 32,760 12 9Leduc Poker (9) 371,809 9,288 221,544 12 17 Leduc Poker (13) 1,179,777 19,656 704,600 12 25 Table 3: Sizes of the games.

Game PCFR + APCFR + SAPCFR +

kuhn Poker 0.0020 0.0033 0.0021 Leduc Poker 0.4252 0.6423 0.4408 Battleship (3,2,3) 50.1909 58.4390 50.8602 Battleship (4,3,2) 103.2314 116.2511 102.4161 Liarâ€™s Dice (4) 0.4327 0.5690 0.4308 Liarâ€™s Dice (5) 3.8178 4.0365 3.7922 Goofspiel (4) 0.0290 0.0393 0.0279 Goofspiel (5) 0.9323 1.9760 0.9430 Subgame 3 278.7428 297.3784 276.9873 Subgame 4 245.1753 254.4792 249.0042 Table 4: Comparison of running times (in minutes).

Dynamics of Î±tI in APCFR +. We now present the dynamics of Î±tI in APCFR +. The experimental results are shown in Figures 4 and 5. Note that we output the average value of Î±tI across all I âˆˆ I . Initially, Î±tI increases rapidly. After approximately 100 iterations, Î±tI stops increasing. A closer examination is that the values of âˆ¥rtI âˆ’ rtâˆ’1

> I

âˆ¥22 and âˆ¥Rt+1

> I

âˆ’ RtI âˆ¥22 are significantly larger in the initial phase than at later stages (as observed by the results shown in below).

Dynamics of PTt=1 âˆ¥rtI âˆ’ rtâˆ’1

> I

âˆ¥22 and PTt=1 âˆ¥Rt+1

> I

âˆ’ RtI âˆ¥22. Experimental results are shown in Figures 6 and 7. We show the cumulative values across all infosets. We observe that PTt=1 âˆ¥rtI âˆ’ rtâˆ’1

> I

âˆ¥22 is at least three times larger than PTt=1 âˆ¥Rt+1

> I

âˆ’ RtI âˆ¥22

for both PCFR + and our algorithms. Intuitively, this phenomenon can be explained by the fact that the upper bound of

âˆ¥Rt+1

> I

âˆ’RtI âˆ¥22 is only a quarter of the upper bound of âˆ¥rtI âˆ’rtâˆ’1

> I

âˆ¥22, which is also used to derive SAPCFR + (as detailed in Section 4.2). Therefore, there must exists a sequence of Î±tI such that PTt=1 ( âˆ¥rtI âˆ’rtâˆ’1

> Iâˆ¥22
> 1+ Î±tI

+ Î±tI âˆ¥Rt+1

> I

âˆ’ RtI âˆ¥22) â‰¤ PTt=1 âˆ¥rtI âˆ’ rtâˆ’1

> I

âˆ¥22,

where the term in the right-hand side is the regret bound for PCFR +. Specifically, let Î±tI = Î± > 0 be a positive constant. To satisfy the inequality PTt=1 ( âˆ¥rtI âˆ’rtâˆ’1

> Iâˆ¥22
> 1+ Î±

+ Î±âˆ¥Rt+1

> I

âˆ’ RtI âˆ¥22) â‰¤ PTt=1 âˆ¥rtI âˆ’ rtâˆ’1

> I

âˆ¥22, it suffices to ensure that PTt=1 Î±âˆ¥Rt+1

> I

âˆ’ RtI âˆ¥22 â‰¤

PTt=1

> Î±âˆ¥rtIâˆ’rtâˆ’1
> Iâˆ¥22
> 1+ Î±

(which comes from PTt=1 Î±âˆ¥Rt+1

> I

âˆ’ RtI âˆ¥22 â‰¤ PTt=1 âˆ¥rtI âˆ’ rtâˆ’1

> I

âˆ¥22 âˆ’ PTt=1

> âˆ¥rtIâˆ’rtâˆ’1
> Iâˆ¥22
> 1+ Î±

), or equivalently,

(1 + Î±) PTt=1 âˆ¥Rt+1

> I

âˆ’ RtI âˆ¥22 â‰¤ PTt=1 âˆ¥rtI âˆ’ rtâˆ’1

> I

âˆ¥22. Given that PTt=1 âˆ¥rtI âˆ’ rtâˆ’1

> I

âˆ¥22 is at least three times larger than

PTt=1 âˆ¥Rt+1

> I

âˆ’ RtI âˆ¥22, we can choose 0 < Î± â‰¤ 2 to ensure (1 + Î±) PTt=1 âˆ¥Rt+1

> I

âˆ’ RtI âˆ¥22 â‰¤ PTt=1 âˆ¥rtI âˆ’ rtâˆ’1

> I

âˆ¥22 holds. Additionally, we find that both PTt=1 âˆ¥rtI âˆ’ rtâˆ’1

> I

âˆ¥22 and PTt=1 âˆ¥Rt+1

> I

âˆ’ RtI âˆ¥22 decrease for our algorithms (APCFR + and SAPCFR +) compared to PCFR +. We speculate that this is due to our algorithmsâ€™ higher stability compared to PCFR +.Specifically, for the reduction of PTt=1 âˆ¥rtI âˆ’ rtâˆ’1

> I

âˆ¥22, when updating using the prediction, our algorithm utilizes smaller learning rates, resulting in smaller gaps between strategies indicated by different explicit accumulated regrets. Intuitively, it leads to smaller value of âˆ¥rtI âˆ’ rtâˆ’1

> I

âˆ¥22. For the reduction of PTt=1 âˆ¥Rt+1

> I

âˆ’ RtI âˆ¥22, we attribute it to that as the learning rate decreases, the gap between the strategies represented by RtI and Ë†RtI also diminishes, which implies the rtI and instantaneous counterfactual regret derived from the strategy represented by RtI becomes closer. This leads to more stable updates from RtI to Rt+1

> I

.Furthermore, we observe that the improvement of our algorithms over PCFR + is correlated with the rate at which the inaccuracy âˆ¥rtI âˆ’ rtâˆ’1

> I

âˆ¥22 between the predicted and observed instantaneous counterfactual regrets decreases in PCFR +. When considering the sizes of the games (Table 3), this inaccuracy decreases particularly rapidly in Battleship (3,2,3) and Goofspiel (4). Specifically, the value of the accumulated inaccuracy PTt=1 âˆ¥rtI âˆ’ rtâˆ’1

> I

âˆ¥22 stops increasing after approximately 10 iterations in only Kuhn Poker, Battleship (3,2,3), and Goofspiel (4), with Kuhn Poker is significantly smaller than Battleship (3,2,3) and Goofspiel (4). According to the experimental results in Figure 3, these games are also the games where our algorithms outperform similar with PCFR +.We also observe that PTt=1 âˆ¥rtI âˆ’ rtâˆ’1

> I

âˆ¥22 and PTt=1 âˆ¥Rt+1

> I

âˆ’ RtI âˆ¥22 exhibit a much higher growth rate during the initial iterations (i.e., when the number of iterations is less than 100) compared to later iterations (i.e., when the number of iterations exceeds 100). This observation aligns with the experimental results on the dynamics of Î±tI , showing that these terms are significantly larger in the early stages of iteration than in later stages.

Dynamics of PTt=1 ( âˆ¥rtI âˆ’rtâˆ’1

> Iâˆ¥22
> 1+ Î±tI

+ Î±tI âˆ¥Rt+1

> I

âˆ’ RtI âˆ¥22). The dynamics of PTt=1 ( âˆ¥rtI âˆ’rtâˆ’1

> Iâˆ¥22
> 1+ Î±tI

+ Î±tI âˆ¥Rt+1

> I

âˆ’ RtI âˆ¥22) for our algorithms and PCFR +, which is related to the upper bound of the counterfactual regret presented in Theorem 4.1, are shown in Figures 8 and 9. These figures show the cumulative values across all infosets. We observe that the values of our algorithms, APCFR + and SAPCFR +, are lower than those of PCFR +. This indicates that our algorithms achieve a lower upper bound on the counterfactual regret. Notably, we find that for most cases, the values of this term are smaller in APCFR + than in SAPCFR +,with exceptions occurring only in Goofspiel (4) and Subgame 4. A lower upper bound on the counterfactual regret implies a faster convergence rate, which is consistent with the experimental results presented in Figure 3 and Table 1.

Dynamics of PTt=1 âˆ¥rtI âˆ’ rtâˆ’1

> I
> 1+ Î±tI

âˆ¥22. The dynamics of PTt=1 âˆ¥rtI âˆ’ rtâˆ’1

> I
> 1+ Î±tI

âˆ¥22 for both our algorithms and PCFR +, which is related to the upper bound on counterfactual regret presented in Theorem B.1, is illustrated in Figures 10 and 11. These figures present the cumulative values across all infosets. For PCFR +, the expressions PTt=1 âˆ¥rtI âˆ’ rtâˆ’1

> I
> 1+ Î±tI

âˆ¥22 and

PTt=1

 âˆ¥rtI âˆ’rtâˆ’1

> Iâˆ¥22
> 1+ Î±tI

+ Î±tI âˆ¥Rt+1

> I

âˆ’ RtI âˆ¥22



are equivalent, as Î±tI = 0 for PCFR +. By synthesizing the results from Fig-ures 8, 9, 10, and 11, we observe that for our algorithms, the value of PTt=1 âˆ¥rtI âˆ’ rtâˆ’1

> I
> 1+ Î±tI

âˆ¥22 significantly exceeds that of

PTt=1

 âˆ¥rtI âˆ’rtâˆ’1

> Iâˆ¥22
> 1+ Î±tI

+ Î±tI âˆ¥Rt+1

> I

âˆ’ RtI âˆ¥22



as the number of iterations increases. Specifically, we observe that the value of

PTt=1 âˆ¥rtI âˆ’ 11+ Î±tI

rtâˆ’1

> I

âˆ¥22 consistently increases over time. In contrast, the value of PTt=1

 âˆ¥rtI âˆ’rtâˆ’1

> Iâˆ¥22
> 1+ Î±tI

+ Î±tI âˆ¥Rt+1

> I

âˆ’ RtI âˆ¥22



tends to stabilize, exhibiting a flattening trend. We hypothesize that this phenomenon arises because, even when rtI and rtâˆ’1

> I

are close, the value of âˆ¥rtI âˆ’ 11+ Î±tI

rtâˆ’1

> I

âˆ¥22 remains significantly large. Consequently, we employ Theorem 4.1 in the main text instead of Theorem B.1.

Empirical convergence rates of APCFR + with an alternative learning approach for Î±t. In Eq (3), we proposed a learning approach for Î±t. Here, we evaluate an alternative learning approach for Î±t, specifically defined as Î±tI =min

r max Ï„ âˆˆ[tâˆ’1] âˆ¥rÏ„I âˆ’rÏ„ âˆ’1

> Iâˆ¥22
> max Ï„âˆˆ[tâˆ’1] âˆ¥RÏ„+1
> Iâˆ’RÏ„Iâˆ¥22

, Î± max



. The experimental results are presented in Figure 12 and Table 5, where the APCFR +

with new learning approach for Î±t is denoted as â€œAPCFR + V2â€. These results demonstrate that this alternative Î±t learning approach generally achieves performance comparable to the method outlined the original APCFR + in most games. We also observe that this alternative Î±t learning approach slightly outperforms the original APCFR + in large games such as Battleship (3,2,3), Battleship (4,3,2), Subgame (3), and Subgame (4). A potential direction for future work is to develop more efficient algorithms for solving large IIGs, building upon the new Î±t learning approach. However, in Liarâ€™s Dice (5), this alternative approach performs significantly worse than the method in Eq. (3).

Comparison with other classical CFR algorithms and the generalization of our approach. Now, we evaluate our algorithms with other classical CFR algorithms such as CFR (Zinkevich et al. 2007), CFR + (Tammelin 2014), and DCFR (Brown and Sandholm 2019a). Initially, we conducted experiments using standard commonly used IIG benchmarks and HUNL Subgames. The experimental results are shown in Figure 13 and Table 6, respectively. We observed that APCFR + and SAPCFR + consistently outperformed CFR and CFR + across all games. However, in poker games like Leduc Poker and HUNL Subgames, APCFR +

and SAPCFR + did not surpass DCFR. Notably, our algorithms and DCFR are not mutually exclusive and can be combined effectively. Specifically, the core idea of our algorithmâ€”the asymmetry of step sizesâ€”can be integrated with that of DCFR, which discounts prior iterations when calculating accumulated regrets. Consequently, we apply the asymmetry mechanism for step sizes to enhance PDCFR + (Xu et al. 2024b), a combination of DCFR and PCFR +, resulting in APDCFR +. We only make evaluation in HUNL Subgames as Leduc Poker is extremely smaller than HUNL Subgames. The experimental results are presented in Table 6. APDCFR + significantly outperforms both PDCFR +, APCFR +, SAPCFR +, and DCFR (note that APCFR +

and SAPCFR + outperform PDCFR +). We also compare APDCFR + with DCFR + (Xu et al. 2024b), an advanced variant of DCFR. The experimental results in Table 6 indicates that APDCFR + also significantly outperforms DCFR +.As mentioned earlier in our main text, we further evaluate the performance of DCFR, PCFR +, APCFR +, SAPCFR +, DCFR +,and APDCFR + on Leduc Poker variants, which is larger than the vanilla Leduc Poker. Detailed experimental results and discussions can be found in the final paragraph of Section 5 in the main text. 10 0 10 1 10 2 10 3

> 1.2
> 1.4
> 1.6
> 1.8
> 2.0
> Value
> kuhn Poker
> 10 010 110 210 3
> 1.0
> 1.2
> 1.4
> 1.6
> 1.8
> 2.0
> 2.2
> 2.4
> 2.6
> Leduc Poker
> 10 010 110 210 3
> 0.4
> 0.6
> 0.8
> 1.0
> 1.2
> Battleship (3,2,3)
> 10 010 110 210 3
> 0.4
> 0.6
> 0.8
> 1.0
> 1.2
> Battleship (4,3,2)
> 10 010 110 210 3
> Iterations
> 0.8
> 1.0
> 1.2
> 1.4
> Value
> Liar's Dice (4)
> 10 010 110 210 3
> Iterations
> 0.8
> 1.0
> 1.2
> 1.4
> 1.6
> Liar's Dice (5)
> 10 010 110 210 3
> Iterations
> 0.6
> 0.8
> 1.0
> 1.2
> 1.4
> Goofspiel (4)
> 10 010 110 210 3
> Iterations
> 0.4
> 0.6
> 0.8
> 1.0
> Goofspiel (5)

Figure 4: Dynamics of Î±tI in standard commonly used IIG benchmarks. Note that, contrary to figures in the main text, the x-axis in this figure is on a logarithmic scale, while the y-axis is not. PCFR + APCFR + SAPCFR + APCFR + V2 S3 1.44e-3 1.02e-3 (-29.2%) 9.44e-4 (-34.4%) 9.84e-4 (-31.7%) S4 1.04e-3 7.53e-4 (-27.6%) 7.83e-4 (-24.7%) 7.38e-4 (-29.0%) Table 5: The final exploitability for PCFR +, APCFR +, SAPCFR +, and APCFR + V2 in HUNL Subgames. Values in red indicate percentages relative to PCFR +.

E Limitations

The primary limitation of APCFR + and SAPCFR + is the theoretical convergence rate of O(1 /âˆšT ). Notably, nearly all CFR algorithms exhibit a theoretical convergence rate of O(1 /âˆšT ). Only a few CFR algorithms have a theoretical convergence rate exceeding O(1 /âˆšT ), e.g. , Reg-CFR and Clairvoyant CFR, which achieve O(1 /T 34 ) and O(1 /T ) theoretical convergence rates, respectively. Unfortunately, as shown in our experiments, the empirical convergence rate of these algorithms fails to surpass that of CFR algorithms, which have a theoretical convergence rate of O(1 /âˆšT ), e.g. , PCFR +. Interestingly, to achieve a faster empirical convergence rate, some CFR algorithms like DCFR, intentionally compromise their theoretical convergence rate. Specifically, the theoretical convergence rate of DCFR is 3 times slower than that of CFR + (under the same number of iterations, the theoretical upper bound on the exploitability of DCFR is three times higher than that of CFR +), as shown in Theorem 3 of Tammelin et al. (2015) (CFR +) and Theorem 2 of Brown and Sandholm (2019a) (DCFR), while DCFR usually outperforms CFR +, as shown in our experiments. 10 0 10 1 10 2 10 3

Iterations

1.45

1.50

1.55

1.60

1.65

1.70

1.75

1.80

1.85

> Value

Subgame3

10 0 10 1 10 2 10 3

Iterations

1.52

1.54

1.56

1.58

1.60

1.62

Subgame4 Figure 5: Dynamics of Î±tI in HUNL Subgames. 10 0 10 1 10 2 10 3

> 0.2
> 0.4
> 0.6
> 0.8
> 1.0
> 1.2
> 1.4
> Value

kuhn Poker

> 10 010 110 210 3
> 5
> 10
> 15
> 20
> 25
> 30

Leduc Poker

> 10 010 110 210 3
> 50
> 100
> 150
> 200
> 250
> 300

Battleship (3,2,3)

> 10 010 110 210 3
> 10
> 20
> 30
> 40
> 50
> 60

Battleship (4,3,2)

> 10 010 110 210 3

Iterations

> 0
> 1
> 2
> 3
> 4
> 5
> Value

Liar's Dice (4)

> 10 010 110 210 3

Iterations

> 0
> 1
> 2
> 3
> 4
> 5

Liar's Dice (5)

> 10 010 110 210 3

Iterations

> 0
> 10
> 20
> 30
> 40
> 50

Goofspiel (4)

> 10 010 110 210 3

Iterations

> 0
> 50
> 100
> 150
> 200
> 250

Goofspiel (5)

PCFR + rtI rt 1

> I22

PCFR + Rt + 1

> I

RtI 22

APCFR + rtI rt 1

> I22

APCFR + Rt + 1

> I

RtI 22

SAPCFR + rtI rt 1

> I22

SAPCFR + Rt + 1

> I

RtI 22

Figure 6: Dynamics of PTt=1 âˆ¥rtI âˆ’ rtâˆ’1

I âˆ¥22 and PTt=1 âˆ¥Rt+1

I âˆ’ RtI âˆ¥22 in standard commonly used IIG benchmarks. CFR CFR + DCFR APCFR + x SAPCFR + x PDCFR +

S3 1.96e-2 1.16e-2 3.05e-4 1.02e-3 9.44e-4 1.08e-3

S4 2.68e-2 1.82e-2 2.18e-4 7.53e-4 7.83e-4 1.10e-3

DCFR + APDCFR +

S3 2.23e-4 (-26.8%) 1.69e-4 (-44.6%) S4 1.55e-4 (-28.9%) 1.27e-4 (-41.7%) Table 6: The final exploitability for CFR, CFR +, and APDCFR + in HUNL Subgames. Values in blue indicate percentages relative to DCFR. â€œS3â€ and â€œS4â€ denote Subgame 3 and Subgame 4, respectively. 10 0 10 1 10 2 10 3

Iterations

0.75

1.00

1.25

1.50

1.75

2.00

2.25

2.50

2.75

> Value

1e11 Subgame3

10 0 10 1 10 2 10 3

Iterations

1

2

3

4

5

6

1e11 Subgame4

PCFR + rtI rt 1

I 22

PCFR + Rt + 1

I RtI 22

APCFR + rtI rt 1

I 22

APCFR + Rt + 1

I RtI 22

SAPCFR + rtI rt 1

I 22

SAPCFR + Rt + 1

I RtI 22Figure 7: Dynamics of PTt=1 âˆ¥rtI âˆ’ rtâˆ’1

I âˆ¥22 and PTt=1 âˆ¥Rt+1

I âˆ’ RtI âˆ¥22 in standard commonly used IIG benchmarks. 10 0 10 1 10 2 10 3

0.4

0.6

0.8

1.0

1.2

1.4

> Value

kuhn Poker

10 0 10 1 10 2 10 3

5

10

15

20

25

30

Leduc Poker

10 0 10 1 10 2 10 3

100

150

200

250

300

Battleship (3,2,3)

10 0 10 1 10 2 10 3

10

20

30

40

50

60

Battleship (4,3,2)

10 0 10 1 10 2 10 3

Iterations

1

2

3

4

5

> Value

Liar's Dice (4)

10 0 10 1 10 2 10 3

Iterations

1

2

3

4

5

Liar's Dice (5)

10 0 10 1 10 2 10 3

Iterations

10

20

30

40

50

Goofspiel (4)

10 0 10 1 10 2 10 3

Iterations

50

100

150

200

250

Goofspiel (5)

PCFR + APCFR + SAPCFR +

Figure 8: Dynamics of PTt=1 ( âˆ¥rtI âˆ’rtâˆ’1

I âˆ¥22

1+ Î±tI

+ Î±tI âˆ¥Rt+1

I âˆ’ RtI âˆ¥22) in standard commonly used IIG benchmarks. 10 0 10 1 10 2 10 3

Iterations

1.4

1.6

1.8

2.0

2.2

2.4

2.6

2.8

> Value

1e11 Subgame3

10 0 10 1 10 2 10 3

Iterations

2

3

4

5

6

1e11 Subgame4

PCFR + APCFR + SAPCFR +Figure 9: Dynamics of PTt=1 ( âˆ¥rtI âˆ’rtâˆ’1

I âˆ¥22

1+ Î±tI

+ Î±tI âˆ¥Rt+1

I âˆ’ RtI âˆ¥22) in HUNL Subgames. 10 0 10 1 10 2 10 3

10 0

10 1

10 2

> Value

kuhn Poker

10 0 10 1 10 2 10 3

10 1

10 2

10 3

Leduc Poker

10 0 10 1 10 2 10 3

10 2

10 3

10 4

Battleship (3,2,3)

10 0 10 1 10 2 10 3

10 1

10 2

10 3

10 4

Battleship (4,3,2)

10 0 10 1 10 2 10 3

Iterations

10 0

10 1

10 2

10 3

> Value

Liar's Dice (4)

10 0 10 1 10 2 10 3

Iterations

10 0

10 1

10 2

10 3

Liar's Dice (5)

10 0 10 1 10 2 10 3

Iterations

10 1

10 2

10 3

10 4

Goofspiel (4)

10 0 10 1 10 2 10 3

Iterations

10 2

10 3

10 4

Goofspiel (5)

PCFR + APCFR + SAPCFR +

Figure 10: Dynamics of PTt=1 âˆ¥rtI âˆ’ rtâˆ’1

I

1+ Î±tI

âˆ¥22 in standard commonly used IIG benchmarks. 10 0 10 1 10 2 10 3

Iterations

10 11

10 12

> Value

Subgame3

10 0 10 1 10 2 10 3

Iterations

10 11

10 12

Subgame4

PCFR + APCFR + SAPCFR +Figure 11: Dynamics of PTt=1 âˆ¥rtI âˆ’ rtâˆ’1

I

1+ Î±tI

âˆ¥22 in HUNL Subgames. 0 1000 2000 3000 4000 5000

10 9

10 7

10 5

10 3

10 1

> Exploitability

kuhn Poker

0 1000 2000 3000 4000 5000

10 4

10 3

10 2

10 1

10 0

Leduc Poker

0 1000 2000 3000 4000 5000

10 8

10 7

10 6

10 5

10 4

10 3

10 2

10 1

10 0

Battleship (3,2,3)

0 1000 2000 3000 4000 5000

10 9

10 8

10 7

10 6

10 5

10 4

10 3

10 2

10 1

Battleship (4,3,2)

0 1000 2000 3000 4000 5000

Iterations

10 8

10 7

10 6

10 5

10 4

10 3

10 2

10 1

10 0

> Exploitability

Liar's Dice (4)

0 1000 2000 3000 4000 5000

Iterations

10 7

10 6

10 5

10 4

10 3

10 2

10 1

10 0

Liar's Dice (5)

0 1000 2000 3000 4000 5000

Iterations

10 8

10 6

10 4

10 2

10 0

Goofspiel (4)

0 1000 2000 3000 4000 5000

Iterations

10 7

10 6

10 5

10 4

10 3

10 2

10 1

10 0

Goofspiel (5)

PCFR + APCFR + SAPCFR + APCFR + V2

Figure 12: Empirical convergence rates of PCFR +, APCFR +, SAPCFR +, and APCFR + V2 in standard commonly used IIG benchmarks. 0 1000 2000 3000 4000 5000

10 9

10 7

10 5

10 3

10 1

> Exploitability

kuhn Poker

0 1000 2000 3000 4000 5000

10 5

10 4

10 3

10 2

10 1

10 0

Leduc Poker

0 1000 2000 3000 4000 5000

10 8

10 7

10 6

10 5

10 4

10 3

10 2

10 1

10 0

Battleship (3,2,3)

0 1000 2000 3000 4000 5000

10 9

10 8

10 7

10 6

10 5

10 4

10 3

10 2

10 1

Battleship (4,3,2)

0 1000 2000 3000 4000 5000

Iterations

10 8

10 7

10 6

10 5

10 4

10 3

10 2

10 1

10 0

> Exploitability

Liar's Dice (4)

0 1000 2000 3000 4000 5000

Iterations

10 7

10 6

10 5

10 4

10 3

10 2

10 1

10 0

Liar's Dice (5)

0 1000 2000 3000 4000 5000

Iterations

10 8

10 6

10 4

10 2

10 0

Goofspiel (4)

0 1000 2000 3000 4000 5000

Iterations

10 7

10 6

10 5

10 4

10 3

10 2

10 1

10 0

Goofspiel (5)

CFR CFR+ DCFR APCFR+ SAPCFR+ Figure 13: Empirical convergence rates of CFR, CFR +, DCFR APCFR +, and SAPCFR + in standard commonly used IIG benchmarks.
